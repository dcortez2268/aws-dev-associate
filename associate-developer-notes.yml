AWS SECURITY TOKEN SERVICE, STS:
  provides identities assuming a role the temporary security credentials they need for access:
  sts:AssumeRole action generates temporary credentials:
  credentials are similar to access keys:
  credentials can be given subset of permissions of permissions policy of role:
  when an identity assumes a role:
    sts:AssumeRole calls are made to STS
    STS requests permissions policy from role and generates credentials based on permissions policy
    STS returns credentials
  temporary credentials contain:
    AccessKeyId, Expiration, SecretAccessKey, SessionToken

  AssumeRoleWithWebIdentity:
    returns a set of temporary security credentials for federeated users who authenticated through a public identity provider, like amazon, google, etc
  AssumeRoleWithSAML:
    returns a set of temporary security credentials for federeated users who authenticated through your organization's identity system that implements SAML2.0, like AD
  

-----------------------------------------------------------------------------------------------:
S3 BUCKET KEYS (know all):
  using S3 SEE-KMS without bucket keys:
    each object uploaded via PUT method uses a unique DEK when using KMS key
    the unique DEK is stored with the object
    limits scalability
  using S3 SEE-KMS with bucket keys:
    KMS key is used to generate a time limited bucket key
    time limited bucket key:
      generates DEKs within s3, offloading the work from KMS and reducing the number of KMS api calls
    increases scalability
    cloudtrail kms event logs now show the bucket ARN instead of object ARN
    works with replication...object encryption is maintained
    not retroactive

-----------------------------------------------------------------------------------------------:
CLOUDWATCH LOGS ARCHITECTURE:
  two sides:
    ingestion side:
      public service that allows you to store, monitor, and access logging data
      natively logs AWS services logs (VPC flow logs, cloudtrail, ebs, ecs, api gateway, lambda, route53, and others)
      CWAgent allows you to log system, custom application, and on premises logging
    subscription side:
      log events:
        a specific occurrence or action that is logged by a service or application
        consist of timestamp and raw message
      log stream:
        a collection of log events that are generated by a single source, such as an EC2 instance
      log group:
        a collection of related log streams
        sets retention (by default logs are stored indefinitely), access permissions, and encryption at rest using KMS
        defines metric filter to analyze patterns within log group and creates a metric
        subscription filter allows real time delivery
      s3 export:
        logs are exported to s3 bucket via CreateExportTask api call
        takes up to 12 hours
        you can encrypt data with only sse-s3 encryption method
      subscription filter:
        allows real time and near real time delivery of log groups
        configures pattern, destination ARN, distribution, access permissions
        near realtime delivery:
          kinesis data firehose allows near realtime delivery
        real time delivery:
          aws managed lambda function natively delivers into elasticsearch in realtime delivery
          custom lambda functions can be used to export data to nearly any destination in realtime
          can deliver to a kinesis data stream in realtime

-----------------------------------------------------------------------------------------------:
S3 REQUESTOR PAYS FEATURE:
  allows requestor of data, instead of bucket owner, to pay the cost of the request:
  unathenticated access is not supported:
  doesn't work with static website hosting or bitTorrent
  bucket level setting
  requesters must add x-amz-request-payer header to confirm payment responsibility

-----------------------------------------------------------------------------------------------:
POLICY INTERPRETATION:
  follow these steps when evaluating policy:
    identify number of statements:
    identify at a high level what each statement does:
    identify overlap of any statements:
    statements with condition field take effect when that condition is true
    'NotAction' field allows all actions besides the actions listed in value, there are a lot of inverses used in policies so be on lookout for those on exam

-----------------------------------------------------------------------------------------------:
POLICY EVALUATION LOGIC:
  for same account:
    aws checks pile of policies in this order:
      explicit denys:
        if it denies, permission is denied and evaluation stops
      organization SCPS on identity's account:
        if it denies, permission is denied and evaluation stops
      resource policies:
        if it allows, permission is allowed and evaluation stops
      IAM identity/permission boundaries:
        permission boundaries allow you to limit permissions granted
        if it denies, permission is denied and evaluation stops
      role/session policies:
        session policies allow you only a subset of role permissions
        if it exists and it denies, permission is denied and evaluation stops
      identity policies:
        access is denied or allowed, or implicit deny
    
    for multi account (account a trying to access account b's resources):
      has same checks as above for account a and also needs a resource policy allowing in account b 

-----------------------------------------------------------------------------------------------:
X-RAY:
  distributed tracing application, it is designed to track sessions through an application:
  aggregrates different services data to give you a single overview of flow of data:
  services sending data into xray require iam permissions:
  tracing header:
    the first service of session generates this which is used to track a request through your distributed application
  segments:
    single block of data sent into xray
    contains host/ip, request, response, work done (times), and issues
  subsegments:
    more granular version of segments
  service graph:
    JSON document that details services and resources which make up your app
  service map:
    visual version of the service graph 
  to integrate with aws ray these services require configuration:
    ec2: xray agent
    ec2: agent in tasks
    lambda: there is an enable option
    beanstalk: agent preinstalled
    api gateway: per stage option
    sns and sqs: there is an enable option

-----------------------------------------------------------------------------------------------:
CI/CD USING AWS CODE: 5:54
  CI/CD stands for Continuous Integration and Continuous Delivery (or Deployment), which are two practices in software development that aim to streamline and automate the process of building, testing, and deploying software.
    Continuous Integration:
      practice where developers frequently merge their code changes into a shared repository, and then automated build and test processes are triggered to ensure that the code is functioning properly and that new changes are not breaking any existing functionality.
    Continuous Development:
      practice where code changes are automatically built, tested, and prepared for release to production environments.

  stages of ci/cd development pipeline:
    code:
      focuses on actual coding process, storage, version control
    build:
      focuses on combining src code, libraries, frameworks, and generates output
    test:
      tests code against expectations
    deploy:
      getting code out to code environments where it will run
    
    without aws:
      manually configure... github (code) -> jenkins (build+test) -> jenkins/other tooling (deploy)
      integration with aws has to be configured
    with aws:
      CodeCommit (code) -> CodeBuild (build+test) -> CodeDeploy (deploy)
      CodePipeline service orchestrates all of these services together
  
  CodePipeline:
    each pipeline has stages
    every pipeline at minimum has source stage which defines where source code is located within specific branch within specific repo
    buildspec.yml:
      collection of build commands and related configuration that CodeBuild implements to run a build
    appspec.yml|json:
      defines exactly how a deployment process proceeds that CodeDeploy implements

  CodeDeploy destinations:
    codeDeploy:
      can be deployed onto one or more ec2 instances using a deployment group
    elastic beanstalk or OpsWorks:
    cloudformation:
      can create/update stacks
    ecs or ecs (blue/green deployment model):
    service catalog or alexa skills kit:
    s3:
    
-----------------------------------------------------------------------------------------------:
CodeCommit:
  main feature is repositories
  authentication to access repo via CL:
    you must have access via HTTPS password or SSH key pair in your IAM security credentials
  authorization for repo:
    access is given via IAM identity policies
  event driven architecture:
    notifications:
      allow you to send notifications to SNS or chatbot targets when events occur
    trigger:
      allow you to trigger SNS or lambda function targets when events occur

-----------------------------------------------------------------------------------------------:
CodePipeline:
  continuous delivery tool that orchestrates CodeCommit, CodeBuild, and CodeDeploy to work together
  pipelines are built from STAGES
  stages can have sequential or parallel ACTIONS
  movement between stages can be automatic or require manual approval
  artifacts:
    files or assets input to an action or output from an action
    input artifacts: s3 -> action
    output artifact: action -> s3
  event driven:
    all state changes -> event bridge

-----------------------------------------------------------------------------------------------:
CodeBuild:
  build as a service product
  fully managed service that allows you to pay for only the resources consumed during builds
  it is an alternative to having a jenkins server that would be running all the time
  used for builds and tests
  uses docker for build environment and can be customized
  integrates with aws services
  architecture:
    gets source from github, codecommit, codepipeline, s3, and more
    buildspec.yml:
      collection of build commands and configs codebuild implements to customize build
      has to be in the root of source folder
      contains environment variables
      contains config for artifacts
      four main phases:
        install:
          install packages in build env, like frameworks and tooling
        pre_build:
          install dependencies and sign into things
        build:
          commands run during the build process
        post_build:
          package things up, push docker image, explicit notifications
    build environment:
      can be customized via buildspec.yml and docker images
    build projects:
      controlled via cli, sdk, and pipeline
  logs: -> s3 and cloudwatch logs
  metrics: -> cloudwatch
  events: -> eventbridge

-----------------------------------------------------------------------------------------------:
CodeDeploy:
  code deployment as a service
  alternative to Jenkins, Ansible, Chef, Puppet, Cloudformation, and more..
  deploys code and prebuilt applications, not resources
  can deploy to EC2, on premise servers, lambda functions, and ECS
  CodeDeploy agent:
    allows you to deploy code to on premises or ec2 instances
  Appspec.yml|json:
    manages deployments
    contains configuration and lifecycle event hooks
    config components:
      files:
        applies to ec2 instances and on premise servers
      permissions: 
        details permissions for files, directories, and folder in files section (applies to ec2 instances and on premise servers)
      resources:
        applies to ecs and lambda
    lifecycle event hooks:
      specifies the event hook for corresponding events, if there is not an event hook for the incoming event then nothing happens
      implementation details are different if triggered on ec2/on premises than if triggered on ecs/lambda
      ApplicationStop:
      DownloadBundle:
      beforeInstall:
      Install:
      AfterInstall:
      ApplicationStart:
      ValidateService:
        verifies that the deployment was successful or not

-----------------------------------------------------------------------------------------------:
SQS EXTENDED CLIENT LIBRARY:
  used when handling messages over SQS max(256KB)
  often used with Java
  implementation of SendMessage api call:
    sends large payloads to s3 and stores a link in message stored in SQS
  (Receive/Delete)Message api call:
    loads/deletes large s3 payload
  
-----------------------------------------------------------------------------------------------:
LAMBDA LAYERS:
  runtimes that allow libraries to be modularized so they can be reused between functions
  libraries are extracted in /opt folder
  types:
    aws layers, custom layers, ARN reference to layer

-----------------------------------------------------------------------------------------------:
LAMBDA CONTAINER IMAGES:
  creates lambda function that can integrate with existing container workflows
  allows you to package code in containers within lambda, and then is added to the ECR
  useful feature for many organizations that use containers and CI/CD processes built for containers
  Lambda Runtime API:
    include this package inside your container images to allow interaction with lambda and your container
  AWS Lambda Runtime Interface Emulator, RIE :
    lets you do local testing to test integration with containers and lambda without using lambda

-----------------------------------------------------------------------------------------------:
LAMBDA AND ALB INTEGRATION:
  architecture of client <-> ALB <-> LAMBDA function req/res:
    ALB translates HTTP/S into a lambda compatible event (JSON structure)
    lambda respons in JSON which is translated back into HTTP/S response
    single value headers... (http//catagram.io?&search=winkie):
      json includes "queryStringParameters" :{ "search" :"winkie"}
    multi-value headers... (http//catagram.io?&search=roffle&search=winkie):
      json includes "multiValueQueryStringParameters" :{ "search" :["winkie", "roffle"]}

-----------------------------------------------------------------------------------------------:
LAMBDA RESOURCE POLICIES:
  define who can invoke or manage your lambda function:
  you can allow or deny certain entities from interacting with your lambda function
  same account permissions:
    either a identity policy/role is required for an entity to access lambda OR a lambda resource policy is required that grants entity permission to access lambda 
  cross account permissions:
    both an identity policy/role is required for an entity to access lambda AND a lambda resource policy is required that grants entity permission to access lambda 
    
-----------------------------------------------------------------------------------------------:
API GATEWAY  HTTP METHODS AND RESOURCES:
  invoke URL:
    contains gateway endpoint dns name, stage, and resource
  methods:
    like HTTP methods, but have different integrations in addition to HTTP..like Lambda, other aws services, mock, and vpc link
  
-----------------------------------------------------------------------------------------------:
API GATEWAY INTEGRATIONS:
  phases of api gateway request/response:
  request phase: 
    client makes a request to api gateway:
    method request:
      defines everything about client's request to the method, (path, headers, parameters )
    integration request:
      handles proxying data to form integration can handle or passes straight through
  integration phase:
    integration receives data, then performs some work, then passes data back to api gateway
  response phase:
    where response is sent to client:
    integration response:
      handles proxying data to form client can handle or passes straight through(headers,status codes, bodies)
    method response:
      handles how communication is delivered back to client 

  different types of integrations:
    mock:
      used for testing, no backend development because everything is handled within api gateway
    http:
      backend http endpoint, requires mapping templates that transform data
    http proxy:
      pass through to integration unmodified, return to the client unmodified (backend needs to use supported format)
    aws:
      lets an api expose aws service actions, requires mapping templates that transform data
    aws proxy (lambda):
      passes through to integration unmodified, returns to client unmodified
  
  mapping templates:
    implements the Velocity Template Language (VTL) to transform data
    common exam question is that this can translate data from SOAP API to REST API

-----------------------------------------------------------------------------------------------:
API Swagger and OpenAPI:
  api gateway can export to OAS and import from OAS:
    api swagger:
      OpenAPI v2
    OpenAPI specification, OAS, v3:
      API description format for RESTful APIs
      defines endpoints, routes, input and output parameters, and authentication methods
  
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK, EB:
  allows you to upload your application, and automatically handles the details of provisioning and managing infrastructure: 
  requires application changes:
  PAAS, platform as a service
  great for small dev teams
  implements cloudformation to provision resources:
    you can customize the infrastructure as much as you want to:
  platforms:
    languages eb supports
    built in languages, docker, and custom platforms:
      go, java se, tomcat, .net core, .net, node.js, php, python, ruby
      single container docker, multicontainer docker, preconfigured docker
      custom platforms via packer
  elastic beanstalk application:
    a container for everything related to an application:
  environments:
    sub containers of application that contain infrastructure and configuration for a specific app version:
    each environment is either a web server tier or a worker tier
    web server tier:
      designed to communicate with end users:
    worker tier:
      designed to process work from web server tier:
    each environment has its own CNAME and a generic DNS name:
    CNAME swap:
      environments can swap their CNAMEs for use cases like green/blue testing:
  versions:
    points at a specific version of deployable code for an application:
    source bundle is stored on s3
  keep databases outside of elastic beanstalk:
    dbs in an ENV are lost if the env is deleted

-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK DEPLOYMENT POLICIES:
  implement how application versions are deployed to environments
  different types:
    all at once:
      deploy application to all ec2 instances at once, and there is outage until whole deployment is complete
      deploys new version to existing ec2 instances
    rolling deployments:
      deploy in rolling batches, there is drop in performance because one batch will always be deploying
      deploys new version to existing ec2 instances
    rolling with additional batch:
      deploys in rolling batches with an additional batch always so capacity is maintained during the process
      deploys new version to existing ec2 instances
    immutable:
      new instances are deployed with new version in a diff asg all at once, once new instances are healthy they replace the old instances in original asg
      deploys new ec2 instances rather than deploying new versions on existing ec2 instances
    traffic splitting:
      immutable, but implements traffic split process so before you replace the old instances you can test with traffic
      deploys new ec2 instances rather than deploying new versions on existing ec2 instances
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK LIFECYCLE AND RDS:
  you can create an RDS instance within an EB environment:
    the instance is then linked to the EB environment
    you should only do this for really small scall dev and test projects
    environment properties:
      RDS_HOSTNAME, RDS_PORT, RDS_DB_NAME, RDS_USERNAME, RDS_PASSWORD
  you can create and RDS instance outside of EB:
    environment properties need to point at RDS instance
    data is outside of the EB env lifecycle
  
  how to decouple existing RDS instance within EB from EB environment:
    create an RDS snapshot
    enable "delete protection"
    create a new eb environment with the same app version
    ensure new environment can connect to the DB, (sg and env properties)
    swap environments (CNAME or DNS)
    terminate the old environment
    locate DELETE_FAILED stack, manually delete and select 'retain stuck resources'

-----------------------------------------------------------------------------------------------:
CUSTOMIZING VIA .EBEXTENSIONS:
  allow you to customize EB environments
  implements Cloudformation to implement changes
  create a .ebextensions folder inside application source bundle:
    add YAML or JSON ending in .config extension
    sections:
      option_settings:
        allows you to set options of resources
      resources:
        allows you to create new resources

-----------------------------------------------------------------------------------------------:
EB and HTTPS:
  you need to apply the SSL cert to the load balancer directly:
    via EB console
    via .ebextensions/securelistener-alb.config, and remember to configure security group

-----------------------------------------------------------------------------------------------:
EB ENVIRONMENT CLONING:
  allows you to create a new environment by cloning an existing one:
  copies all changes you've made to environment via ebs console, but not unmanaged changes you've made via cli or api:
  when copying RDS, the instance is copied but data is not
  different platform branches (languages eb supports) are not guaranteed to be compatible
  you can clone via eb console, API, or "eb clone environmentName"

-----------------------------------------------------------------------------------------------:
EB AND DOCKER:
  single container mode:
    implement when you want to only run one container on one docker host per environment
    uses ec2 with docker, not ECS
    referred to as docker in console UI
    you provide 1 of these:
      dockerfile:
        eb will build a docker image and use this to run a container
      Dockerrun.aws.json (version 1):
        configures what image to use, ports, volumes, and other attributes
      docker-compose.yml:
        if you use docker dompose
  multi-container mode:
    implements multiple containers running on one or more ec2 instances per environment
    implements an elb
    creates an ecs cluster:
      contains ec2 instances with containers 
    requires a dockerrun.aws.json (v2) file:
      in application source bundle
    requires images to be stored in a container registry such as ECR:

-----------------------------------------------------------------------------------------------:
PRACTICE TEST QUESTIONS:

to include lambda code within a cfn template:
  implement the ZipFile key

if s3 bucket is timing out when trying to list items:
  implement max-items parameter or page-size parameter to reduce number of objects returned   

  
     