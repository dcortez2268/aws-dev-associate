AWS SECURITY TOKEN SERVICE, STS:
  provides identities assuming a role the temporary security credentials they need for access:
  sts:AssumeRole action generates temporary credentials:
  credentials are similar to access keys:
  credentials can be given subset of permissions of permissions policy of role:
  when an identity assumes a role:
    sts:AssumeRole calls are made to STS
    STS requests permissions policy from role and generates credentials based on permissions policy
    STS returns credentials
  temporary credentials contain:
    AccessKeyId, Expiration, SecretAccessKey, SessionToken

  AssumeRoleWithWebIdentity api call:
    returns a set of temporary security credentials for federeated users who authenticated through a public identity provider, like amazon, google, etc
  AssumeRoleWithSAML api call:
    returns a set of temporary security credentials for federeated users who authenticated through your organization's identity system that implements SAML2.0, like AD
  GetSessionToken api call:
    returns a set of temporary security credentials for an aws account or iam user.  you use it if you want to require MFA to allow only certain devices to access specific aws api operations
    aws sts get-session-token [options]:

  sts:DecodeAuthorizationMessage:
    allows a user to decode an encoded error message
    aws sts decode-authorization-message --encoded-message 'message':

-----------------------------------------------------------------------------------------------:
S3 BUCKET KEYS (know all):
  using S3 SEE-KMS without bucket keys:
    each object uploaded via PUT method uses a unique DEK when using KMS key
    the unique DEK is stored with the object
    limits scalability
  using S3 SEE-KMS with bucket keys:
    KMS key is used to generate a time limited bucket key
    time limited bucket key:
      generates DEKs within s3, offloading the work from KMS and reducing the number of KMS api calls
    increases scalability
    cloudtrail kms event logs now show the bucket ARN instead of object ARN
    works with replication...object encryption is maintained
    not retroactive

-----------------------------------------------------------------------------------------------:
CLOUDWATCH LOGS ARCHITECTURE:
  two sides:
    ingestion side:
      public service that allows you to store, monitor, and access logging data
      natively logs AWS services logs (VPC flow logs, cloudtrail, ebs, ecs, api gateway, lambda, route53, and others)
      CWAgent allows you to log system, custom application, and on premises logging
    subscription side:
      log events:
        a specific occurrence or action that is logged by a service or application
        consist of timestamp and raw message
      log stream:
        a collection of log events that are generated by a single source, such as an EC2 instance
      log group:
        a collection of related log streams
        sets retention (by default logs are stored indefinitely), access permissions, and encryption at rest using KMS
        defines metric filter to analyze patterns within log group and creates a metric
        subscription filter allows real time delivery
      s3 export:
        logs are exported to s3 bucket via CreateExportTask api call
        takes up to 12 hours
        you can encrypt data with only sse-s3 encryption method
      subscription filter:
        allows real time and near real time delivery of log groups
        configures pattern, destination ARN, distribution, access permissions
        near realtime delivery:
          kinesis data firehose allows near realtime delivery
        real time delivery:
          aws managed lambda function natively delivers into elasticsearch in realtime delivery
          custom lambda functions can be used to export data to nearly any destination in realtime
          can deliver to a kinesis data stream in realtime

-----------------------------------------------------------------------------------------------:
S3 REQUESTOR PAYS FEATURE:
  allows requestor of data, instead of bucket owner, to pay the cost of the request:
  unathenticated access is not supported:
  doesn't work with static website hosting or bitTorrent
  bucket level setting
  requesters must add x-amz-request-payer header to confirm payment responsibility

-----------------------------------------------------------------------------------------------:
POLICY INTERPRETATION:
  follow these steps when evaluating policy:
    identify number of statements:
    identify at a high level what each statement does:
    identify overlap of any statements:
    statements with condition field take effect when that condition is true
    'NotAction' field allows all actions besides the actions listed in value, there are a lot of inverses used in policies so be on lookout for those on exam

-----------------------------------------------------------------------------------------------:
POLICY EVALUATION LOGIC:
  for same account:
    aws checks pile of policies in this order:
      explicit denys:
        if it denies, permission is denied and evaluation stops
      organization SCPS on identity's account:
        if it denies, permission is denied and evaluation stops
      resource policies:
        if it allows, permission is allowed and evaluation stops
      IAM identity/permission boundaries:
        permission boundaries allow you to limit permissions granted
        if it denies, permission is denied and evaluation stops
      role/session policies:
        session policies allow you only a subset of role permissions
        if it exists and it denies, permission is denied and evaluation stops
      identity policies:
        access is denied or allowed, or implicit deny
    
    for multi account (account a trying to access account b's resources):
      has same checks as above for account a and also needs a resource policy allowing in account b 

-----------------------------------------------------------------------------------------------:
AWS CLI CREDENTIALS PROVIDER CHAIN:
  cli will look for credentials in this order:
    1. command line options:
    2. environment variables:
    3. cli credentials file:
    4. cli configuration file:
    5. container credentials:
    6. instance profile credentials:

-----------------------------------------------------------------------------------------------:
tracing: 
  end to end way to follow a request
instrumentation:
  measure of a product's performance, diagnose errors, and to write trace information
X-RAY:
  distributed tracing application, it is designed to track sessions through an application:
  aggregrates different services data to give you a single overview of flow of data:
  tracing header:
    the first service of session generates this which is used to track a request through your distributed application
  trace:
    composed of all the segments generated by a single request
    maximum size is 500kb
    trace segment:
      json representation of trace
  segments:
    single block of data sent into xray that details requests within your application
    contains host/ip, request, response, work done (times), and issues
  subsegments:
    more granular version of segments, and provide additional details about a call to an aws service, an external HTTP API, or a sql database
    can instrument specific functions or lines of code in your application
    namespace field:
      set to "aws" for aws sdk calls and to "remote" for other downstream calls
    annotations:
      simple key value pairs that index traces and use with filter expressions.
      used to group traces
  service graph:
    JSON document that details services and resources which make up your app
    retained for 30 days
  service map:
    visual version of the service graph 
  filter expressions:
    find traces related to specific paths or users
  metadata:
    more complex key value pairs with values of any type, but are not indexed.  Record data that you want to store in the trace but you don't need to use for searching traces
  x-ray daemon:
    collects segments for multiple requests and uploads them to xray in one api call
    lightweight process that runs on each EC2 instance in your application's environment
  x-ray agent:
     a software library that you can integrate into your application's code. 
  sampling:
    records a portion of the requests rather than tracing every request
    reduces cost and amount of data you record
  integrates with:
    lambda
    beanstalk
    ecs
    elb
    api gateway
    ec2 instances or any application server, even on premise
  how to configure?:
    your code must import the x-ray sdk
    you must install the x-ray daemon or enable x-ray aws integration for some services that will run the xray daemon for you
  xray troubleshooting:
    services sending data into xray require iam write permissions:
    ensure xray daemon is running:
  to integrate with aws ray these services require configuration:
    ec2: 
      xray agent needs to be imported to run application code
      xray daemon needs to be installed
      needs to have instance role with proper access
    ecs:
      xray agent in tasks
      create a docker image that runs the xray daemon, upload it to a docker repo, and deploy to your ecs cluster
      to allow communication with xxray daemon, allow traffic on UPD port 2000 in your task definition file
    lambda: 
      there is an 'enable aws x-ray' checkbox, installs xray agent
      make sure your application code is instrumented with the xray sdk
    beanstalk:
      daemon can be installed by setting an option in eb console or with a config file in .ebextensions/xray-daemon.config
      agent is preinstalled
      make sure your application code is instrumented with the xray sdk and that ec2 instance has access
      xray daemon is not provided for multicontainer docker, you need to configure yourself
    api gateway: per stage option
    sns and sqs: there is an enable option
    alb: adds trace id to the request header before sending it to target group

  xray APIs:
    writes:
      PutTraceSegments:
        you can send segment documents directly to xray by using the PutTraceSegments api
        if segments aren't natively supported by the service, you can implement custom inferred segments by including subsegments
      PutTelemetryRecords: used by the aws xray daemon to upload telemetry
      GetSamplingRules:
        gets all of the sampling rules, and updates the daemons
    reads:
      BatchGetTraces:
        retrieves a list of traces specified by id
      GetTraceSummaries:
        retrieves ids and annotations for traces available for a specified time frame.  Pass the trace IDs to BatchGetTraces to get full traces
      GetTraceGraph:
        retrieves a service graph for one or more specific trace ids
      GetServiceGraph:
        gets service graph
  

  


  x-ray fetches the client ip address via:
    the 'X-forwarded-for' header of the request

  lambda and xray environment variables:
  _X_AMZN_TRACE_ID:
    contains tracing header
  AWS_XRAY_CONTEXT_MISSING:
    contains behavior if function tries to record x-ray data but a tracing header is not available
  AWS_XRAY_DAEMON_ADDRESS:
    contains x-ray daemon's address and allows direct communication with the daemon vs implementing sdk
  

-----------------------------------------------------------------------------------------------:
CI/CD USING AWS CODE: 5:54
  CI/CD stands for Continuous Integration and Continuous Delivery (or Deployment), which are two practices in software development that aim to streamline and automate the process of building, testing, and deploying software.
    Continuous Integration:
      practice where developers frequently merge their code changes into a shared repository, and then automated build and test processes are triggered to ensure that the code is functioning properly and that new changes are not breaking any existing functionality.
    Continuous Development:
      practice where code changes are automatically built, tested, and prepared for release to production environments.

  stages of ci/cd development pipeline:
    code:
      focuses on actual coding process, storage, version control
    build:
      focuses on combining src code, libraries, frameworks, and generates output
    test:
      tests code against expectations
    deploy:
      getting code out to code environments where it will run
    
    without aws:
      manually configure... github (code) -> jenkins (build+test) -> jenkins/other tooling (deploy)
      integration with aws has to be configured
    with aws:
      CodeCommit (code) -> CodeBuild (build+test) -> CodeDeploy (deploy)
      CodePipeline service orchestrates all of these services together
  
  CodePipeline:
    each pipeline has stages
    every pipeline at minimum has source stage which defines where source code is located within specific branch within specific repo
    buildspec.yml:
      collection of build commands and related configuration that CodeBuild implements to run a build
    appspec.yml|json:
      defines exactly how a deployment process proceeds that CodeDeploy implements

  CodeDeploy destinations:
    codeDeploy:
      can be deployed onto one or more ec2 instances using a deployment group
    elastic beanstalk or OpsWorks:
    cloudformation:
      can create/update stacks
    ecs or ecs (blue/green deployment model):
    service catalog or alexa skills kit:
    s3:
    
-----------------------------------------------------------------------------------------------:
CodeCommit:
  main feature is repositories
  authentication to access repo via CL:
    SSH key pair: 
      in your IAM security credentials (public key is in developer's iam user and private is in repo)
    https:
      git cli credential helper:
        requires an aws credential profile, which stores access key id and secret access key
        aws cli includes this helper by default
      generate https git credentials and store in git credential manager:
        you configure the git credentials manually

  authorization for repo:
    access is given via IAM identity policies
  event driven architecture:
    notifications:
      allow you to send notifications to SNS or chatbot targets when events occur
    trigger:
      allow you to trigger SNS or lambda function targets when events occur

-----------------------------------------------------------------------------------------------:
CodePipeline:
  continuous delivery tool that orchestrates CodeCommit, CodeBuild, and CodeDeploy to work together
  pipelines are built from STAGES
  stages can have sequential or parallel ACTIONS
  movement between stages can be automatic or require manual approval and implements topic
  artifacts:
    files or assets input to an action or output from an action
    input artifacts: s3 -> action
    output artifact: action -> s3
  event driven:
    all state changes -> event bridge

-----------------------------------------------------------------------------------------------:
CodeBuild:
  build as a service product
  fully managed service that allows you to pay for only the resources consumed during builds
  it is an alternative to having a jenkins server that would be running all the time
  used for builds and tests before deployment:
  uses docker for build environment and can be customized
  integrates with aws services
  architecture:
    gets source from github, codecommit, codepipeline, s3, and more
    buildspec.yml:
      collection of build commands and configs codebuild implements to customize build:
      has to be in the root of source folder:
      contains environment variables:
        can reference parameter store parameters or secrets manager secrets
      contains config for artifacts, you can enable dependencies caching in s3:
      four main phases:
        install:
          install packages in build env, like frameworks and tooling
        pre_build:
          install dependencies and sign into things
        build:
          commands run during the build process
        post_build:
          package things up, push docker image, explicit notifications
    build environment:
      can be customized via buildspec.yml and docker images
    build projects:
      controlled via cli, sdk, and pipeline
  logs: -> s3 and cloudwatch logs
  metrics: -> cloudwatch
  events: -> eventbridge

  to build with a proxy server:
    implement codebuild in a private subnet and a proxy server in a public subnet
    implement ssl-bump, buildspec.yml with proxy element

-----------------------------------------------------------------------------------------------:
CodeDeploy:
  code deployment as a service
  alternative to Jenkins, Ansible, Chef, Puppet, Cloudformation, and more..
  deploys code and prebuilt applications, not resources:
  can deploy to EC2, on premise servers, lambda functions, and ECS:
  implements rollbacks and can trigger cloudwatch alarm:
  CodeDeploy agent:
    required to deploy code to on premise servers:
  Appspec.yml|json:
    implements how deployments will happen:
    contains configuration and lifecycle event hooks
    config components:
      files:
        applies to ec2 instances and on premise servers
      permissions: 
        details permissions for files, directories, and folder in files section (applies to ec2 instances and on premise servers)
      resources:
        applies to ecs and lambda
    lifecycle event hooks:
      specifies the event hook for corresponding events, if there is not an event hook for the incoming event then nothing happens
      implementation details are different if triggered on ec2/on premises than if triggered on ecs/lambda
      ec2 / on prem hooks:
        ApplicationStop:
        DownloadBundle:
        beforeInstall:
        Install:
        AfterInstall:
        ApplicationStart:
        ValidateService:
          verifies that the deployment was successful or not
      lambda/ecs hooks:
        start:
        beforeAllowTraffic:
        AllowTraffic:
        AfterAllowTraffic:
        end:
    deployment configurations:
      (ec2/on prem):
        in place:
          turns off all existing instances, latest version is installed, and instance is restarted
        blue/green:
          manually create a completely new environment and switch cnam for alb
          AllAtOnce: most downtime
          HalfAtATime: reduced capacity by 50%
          OneAtATime: slowest, lowest availability impact
          custom: define your %
      (lambda/ecs):  
        traffic shift (blue/green):
          canary:
            traffic is shifted into two increments
            immediately first increment of traffic shifted, then the last increment after time period
          linear:
            traffic is shifted in equal increments with an equal number of time between each increment
          all at once:
            all traffic is shifted all at once
    deployment groups:
      allows you to deploy an application revision to different sets of instances at different times:
      can apply to individual instances or an autoscaling group:
    
-----------------------------------------------------------------------------------------------:
SQS EXTENDED CLIENT LIBRARY:
  used when handling messages over SQS max(256KB)
  Java library
  implementation of SendMessage api call:
    sends large payloads to s3 and stores a pointer link in message stored in SQS
  (Receive/Delete)Message api call:
    loads/deletes large s3 payload
  
-----------------------------------------------------------------------------------------------:
LAMBDA LAYERS:
  zip archives that allow runtimes, libraries, or other dependencies to be modularized so they can be reused between functions
  libraries are extracted in /opt folder
  types:
    aws layers, custom layers, ARN reference to layer

-----------------------------------------------------------------------------------------------:
LAMBDA CONTAINER IMAGES:
  creates lambda function that can integrate with existing container workflows
  allows you to package code in containers within lambda, and then is added to the ECR
  useful feature for many organizations that use containers and CI/CD processes built for containers
  Lambda Runtime API:
    include this package inside your container images to allow interaction with lambda and your container
  AWS Lambda Runtime Interface Emulator, RIE :
    lets you do local testing to test integration with containers and lambda without using lambda

-----------------------------------------------------------------------------------------------:
LAMBDA AND ALB INTEGRATION:
  architecture of client <-> ALB <-> LAMBDA function req/res:
    ALB translates HTTP/S into a lambda compatible event (JSON structure)
    lambda respons in JSON which is translated back into HTTP/S response
    single value headers... (http//catagram.io?&search=winkie):
      json includes "queryStringParameters" :{ "search" :"winkie"}
    multi-value headers... (http//catagram.io?&search=roffle&search=winkie):
      json includes "multiValueQueryStringParameters" :{ "search" :["winkie", "roffle"]}

-----------------------------------------------------------------------------------------------:
LAMBDA FUNCTION URLS:
  allow you to configure a HTTPS endpoint in front of your lambda function without having to configure additional services besides Lamda (like api gateway)
  can be publicly accessible or require iam authentication

-----------------------------------------------------------------------------------------------:
LAMBDA RESOURCE POLICIES:
  define who can invoke or manage your lambda function:
  you can allow or deny certain entities from interacting with your lambda function
  same account permissions:
    either a identity policy/role is required for an entity to access lambda OR a lambda resource policy is required that grants entity permission to access lambda 
  cross account permissions:
    both an identity policy/role is required for an entity to access lambda AND a lambda resource policy is required that grants entity permission to access lambda 
    
-----------------------------------------------------------------------------------------------:
LAMBDA CONTEXT OBJECT:
  object that is passed to handler in lambda function by the lambda service:
  provides methods and properties that provide information about the invocation, function, and execution environment:

-----------------------------------------------------------------------------------------------:
API GATEWAY  HTTP METHODS AND RESOURCES:
  invoke URL:
    contains gateway endpoint dns name, stage, and resource
  methods:
    like HTTP methods, but have different integrations in addition to HTTP..like Lambda, other aws services, mock, and vpc link
  
-----------------------------------------------------------------------------------------------:
API GATEWAY USAGE PLAN: (know all)
  allows you to specify who can access one ore more deployed api stages and methods
  allows you to configure throttling limits and quota limits that are enforced on customer api keys

-----------------------------------------------------------------------------------------------:
API GATEWAY INTEGRATIONS:
  phases of api gateway request/response:
  request phase: 
    client makes a request to api gateway:
    method request:
      defines everything about client's request to the method, (path, headers, parameters )
    integration request:
      handles proxying data to form integration can handle or passes straight through
  integration phase:
    integration receives data, then performs some work, then passes data back to api gateway
  response phase:
    where response is sent to client:
    integration response:
      handles proxying data to form client can handle or passes straight through(headers,status codes, bodies)
    method response:
      handles how communication is delivered back to client 

  different types of integrations:
    mock:
      used for testing, no backend development because everything is handled within api gateway
    http:
      backend http endpoint, requires mapping templates that transform data
    http proxy:
      pass through to http integration unmodified, return to the client unmodified (backend needs to use supported format)
    aws:
      lets an api expose aws service actions, requires mapping templates that transform data
    aws proxy (lambda):
      passes through to aws integration unmodified, returns to client unmodified
  
  mapping templates:
    implements the Velocity Template Language (VTL) to transform data
    common exam question is that this can translate data from SOAP API to REST API

-----------------------------------------------------------------------------------------------:
API Swagger and OpenAPI:
  api gateway can export to OAS and import from OAS:
    api swagger:
      OpenAPI v2
    OpenAPI specification, OAS, v3:
      API description format for RESTful APIs
      defines endpoints, routes, input and output parameters, and authentication methods
  
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK, EB:
  allows you to upload your application, and automatically handles the details of provisioning and managing infrastructure: 
  requires application changes:
  PAAS, platform as a service
  great for small dev teams
  implements cloudformation to provision resources:
    you can customize the infrastructure as much as you want to:
  platforms:
    languages eb supports
    built in languages, docker, and custom platforms:
      go, java se, tomcat, .net core, .net, node.js, php, python, ruby
      single container docker, multicontainer docker, preconfigured docker
      custom platforms via packer
  elastic beanstalk application:
    a container for everything related to an application:
  environments:
    sub containers of application that contain infrastructure and configuration for a specific app version:
    each environment is either a web server tier or a worker tier
    web server tier:
      designed to communicate with end users:
    worker tier:
      designed to process work from web server tier:
      manages a amazon sqs queue and run a daemon process on instances that reads from the queue for you:
      includes asg, ec2 instances, and iam role
    each environment has its own CNAME and a generic DNS name:
  CNAME swap:
    environments can swap their CNAMEs for use cases like green/blue testing:
  versions:
    points at a specific version of deployable code for an application:
    source bundle is stored on s3:
  keep databases outside of elastic beanstalk:
    dbs in an ENV are lost if the env is deleted

-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK DEPLOYMENT POLICIES:
  implement how application versions are deployed to environments
  different types:
    all at once:
      deploy application to all ec2 instances at once, and there is outage until whole deployment is complete
      deploys new version to existing ec2 instances
    rolling deployments:
      deploy in rolling batches, there is drop in performance because one batch will always be deploying
      deploys new version to existing ec2 instances
    rolling with additional batch:
      deploys in rolling batches with an additional batch always so capacity is maintained during the process
      deploys new version to existing ec2 instances
    immutable:
      new instances are deployed with new version in a diff asg all at once, once new instances are healthy they replace the old instances in original asg
    traffic splitting:
      canary testing.. identical to immutable but before you replace old instances with new you can test a % of traffic to new deployment
    blue/green:
      not a direct feature of eb
      manually create a completely new environment and switch over when ready by swapping urls done
      route 53 can be setup using a weighted policy to test new environment before swapping

    use the eb cli to deploy:
      eb deploy 
      generated via a zip or war file in .elasticbeanstalk/config.yml in project folder
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK LIFECYCLE AND RDS:
  you can create an RDS instance within an EB environment:
    the instance is then linked to the EB environment
    you should only do this for really small scall dev and test projects
    environment properties:
      RDS_HOSTNAME, RDS_PORT, RDS_DB_NAME, RDS_USERNAME, RDS_PASSWORD
  you can create and RDS instance outside of EB:
    environment properties need to point at RDS instance
    data is outside of the EB env lifecycle
  
  how to decouple existing RDS instance within EB from EB environment:
    create an RDS snapshot
    enable "delete protection"
    create a new eb environment with the same app version
    ensure new environment can connect to the DB, (sg and env properties)
    swap environments (CNAME or DNS)
    remove sg rule from old environment
    terminate the old environment
    locate DELETE_FAILED stack, manually delete and select 'retain stuck resources'

  beanstalk lifecycle policies:
    can be based on:
      time, space
    can store at most 1000 versions
    option not to delete the source bundle in s3 to prevent data loss

-----------------------------------------------------------------------------------------------:
CUSTOMIZING VIA .EBEXTENSIONS:
  allow you to customize EB environments:
  implements Cloudformation to implement changes:
  requirements:
    .ebextensions/ directory in the root of source code:
    extensions file needs to be in YAML or JSON format with .config extension:
    sections:
      option_settings:
        allows you to set options of resources
      resources:
        allows you to create new resources
        resources managed by this file will be deleted if environment is deleted
-----------------------------------------------------------------------------------------------:
EB and HTTPS:
  you need to apply the SSL cert to the load balancer directly:
    via EB console
    via .ebextensions/securelistener-alb.config, and remember to configure security group

-----------------------------------------------------------------------------------------------:
EB ENVIRONMENT CLONING:
  allows you to create a new environment by cloning an existing one:
  copies all changes you've made to environment via ebs console, but not unmanaged changes you've made via cli or api:
  you can clone via eb console, API, or "eb clone environmentName":
  when copying RDS, the instance is copied but data is not
  different platform branches (languages eb supports) are not guaranteed to be compatible

-----------------------------------------------------------------------------------------------:
EB AND DOCKER:
  single container mode:
    implement when you want to only run one container on one docker host per environment
    uses ec2 with docker, not ECS
    referred to as docker in console UI
    you provide 1 of these:
      dockerfile:
        eb will build a docker image and use this to run a container
      Dockerrun.aws.json (version 1):
        configures what image to use, ports, volumes, and other attributes
      docker-compose.yml:
        if you use docker dompose
  multi-container mode:
    implements multiple containers running on one or more ec2 instances per environment
    implements an elb
    creates an ecs cluster:
      contains ec2 instances with containers 
    requires a dockerrun.aws.json (v2) file:
      in application source bundle and creates an environment
    requires images to be stored in a container registry such as ECR:

-----------------------------------------------------------------------------------------------:
env.yaml:
  environment configurations for elastic beanstalk
cron.yaml:
  you can define periodic tasks and add to source bundle to add jobs to your worker's environment's queue automatically

-----------------------------------------------------------------------------------------------:
PRACTICE TEST QUESTIONS:

to include lambda code within a cfn template:
  Zipfile (field inside AWS::Lambda::Function resource): 
    "code here"

if s3 bucket is timing out when trying to list items:
  implement max-items parameter or page-size parameter to reduce number of objects returned:
to implement more efficient reads on large dynamodb tables:
  reduce page size and implement query operations   
if cli script to list items is timing out:
  add pagination parameters, -max-items and -starting-token:

if you are getting 403 error when trying to upload large file with kms key to s3 bucket:
  check to ensure you have the kms:Decrypt and kms:GenerateDataKey* actions because these permissions are required when s3 must decrypt and read data to complete the multipart upload

KCL, Kinesis Client Library:
  ensures there is a record processor to process each shard
  also tracks shards in the stream using a dynamodb table
  ec2 instances contain a record processer and one KCL worker
  number of instances should not exceed the number of shards, except for failure standy purposes:
  number of instances can be less than number of shards because one worker can process multiple shards:

how to give access to server in an on premise environment trying to access aws services:
  create a new IAM user with programmatic access via access keys
  create the credential file at ~/.aws/credentials
  it is always best practice to use role with ec2 instance, however when server is on prem then it can only access aws services via access keys

projection expression:
  allows you to get just some of dynamodb attributes rather than all of them when reading data from table
filter expressions:
  allows you to get just some of dynamodb items within the results returned to you when reading data from table
error retries and exponential backoff:
  uses progressively longer waits between retries for consecutive error responses

Exponential backoff:
  use whenever you get ThrottlingExceptionError:
    you'll get this error when you use too many api calls for your api limits
  functionality is automatically implemented in sdk api calls
  implement yourself when:
    you receive 500 server errors
    throttling issues

AWS SAM, serverless application model:
  toolkit that improves the developer experience of building and running serverless applications on aws
  benefits:
    defines your application infrastructure code quickly, using less code
    manage applications through their entire dev lifecycle
    quickly provision permissions between resources with sam connectors
    manage your terraform serverless applications
  to allow cloudformation to use SAM syntax in template:
    include Transform section:
      specifies the verions to use
  architecture:
    sam template specification:
      framework that you can use to define your serverless application infrastructure on AWS
      you can implement both the cloudformation and sam syntax within the same template
      implements cloudformation
    sam cli:
      command line tool used with sam templates and third party integrations such as terraform to build and run your serverless apps
      sam init:
        initilizes a serverless application with an aws sam template
      sam build:
        builds any dependencies that your application has
      sam package:
        this command zips your code artifacts, uploads them to s3, and produces a packaged aws sam template file that's ready to be used
      sam deploy:
        this command uses the sam package file to deploy your application
    alternative to sam package and sam deploy:
      aws cloudformation package and aws cloudformation deploy
    aws serverless application repository, SAR:
      allows you to share your serverless applications packages using SAM with other aws accounts
    sam cli + aws toolkits:
      allows you to debug your lambda fns locally and execute code line by line

errors you might see when creating lambda functions via cli with CreateFunction API:
  InvalidParameterValueException:
    one of the parameters in the request is invalid
  ServiceException:
    the lambda service encountered an internal error

EventBridge rules and dynamodb:
  rules are not capable of detecting table level events from dynamodb


KMS ENCRYPTION PATTERNS AND ENVELOPE ENCRYPTION:
  when data is < 4kb in size:
    encrypt api:
      encrypts plaintext into ciphertext by implementing CMK in KMS
    decrypt api:
      decrypts ciphertext into plaintext by implementing CMK in KMS

  envelope encryption:
    implement when data is greater than 4kb in size, when you need to implement DEK
    process to encrypt data:
      generateDataKey api:
        returns a plaintext copy of the data key along with a copy of the encrypted data key
        useful when you need to encrypt data immediately
        plaintext key is used to encrypt data, is discarded, and then encrypted data is stored with encrypted data key
      generateDataKeyWithoutPlaintext api:
        returns only a copy of the encrypted data key
        useful when you need to encrypt data at some point, but not immediately
        decrypt api:
          decrypts the data key and returns a plaintext of the data copy key
    process to decrypt data:
      decrypt api:
        returns a plaintext copy of the encrypted data key stored with encrypted data
        plaintext key is used to decrypt data and encrypted data key and plaintext copy are discarded 
    top level plaintext key:
      master key
    data key caching:
      allows you to re-use data keys instead of creating new ones for each encryption
      the aws encryption key sdk implements data key caching

  s3 bucket key:
    implemented with ss3-kms encryption
    reduces number of api calls made to kms by 99%
    kms creates a s3 bucket key that is used to create new data keys used for encrypting s3 objects INSTEAD of calling KMS every time to create new data key 


how to invalidate api gateway cache (know all):
  from the ui:
    can be invalidated immediately
  client can invalidate cache via 'cache-control: max-age=0' header:
    allows you to bypass cache and retrieve data from source
    needs proper authorization:
      select the 'require authorization' checkbox in the cache settings of your api via the console
      execute-api:InvalidateCache action:
        allows iam role to make authorized invalidate cache requests to the api

aws_iam method authorization type for api gateway:
  allows you to limit the api to only be accessed by iam users or roles

lambda concurrent executions:
  refers to the number of executions of your function code that is happening at same time
  push based event sources:
    concurrent executions = (invocations per second) x (average execution duration in seconds)
    by default the limit per region is 1000 concurrent executions
    aws enforces the unreserved concurrency pool will always be at least 100 concurrent executions 
  poll based event sources:
    concurrent executions = at most the number of shards in a kinesis data stream

https status code errors in api gateway:
  504:
    integration failure, integration timeout, the integration timeout lasts from 50ms to 29 seconds for all integration types
  502 or 429:
    too many requests and api gateway is throttling

dynamodb errors:
  ProvisionedThroughputExceededException:
    not enough RCU or WCU left for operation
  ThrottlingException:
    rate of requests exceeds the allowed throughput
  RequestLimitExceeded:
    provisioned RCU or WCU (throughput) exceeds current throughput limit for account

dynamodb operations:
  BatchGetItem:
    allows you to get multiple items at once in parallel
  BatchWriteItem:
    allows you to write multiple items at once in parallel
  UpdateItem:
    allows you to perform write requests
    you can implement an atomic counter
  TransactWriteItems or TransactGetItems:
    allows you to group multiple actions together and submit them as a single all-or-nothing operation
  for each of these operations you can include 'ReturnConsumedCapacity' argument:
    to return the number of wcu consumed by the operation
    additional arguments:
      total:
        returns the total number of wcus consumed
      indexes:
        returns the total number of wcus consumed, with subtotals for the table and any secondary indexes that were affected by operation
      none:
        no wcu details are returned (default)
  table cleanup:
    scan and deleteItem:
      very slow and is expensive as it consumes rcu and wcu
    drop table and recreate table:
      recommended as it is fast and efficient
  copying a dynamodb table:
    backup and restore into a new table:
      recommended but takes some time
    use aws data pipeline:
    scan and implement PutItem or BatchWriteItem:
      implement your own code  
  
dynamodb write types:
  concurrent:
    sequential writes overwrite preceding write
  conditional writes:
    each item has an attribute that acts as a version number and you can update item only if the version number on the server side has not changed
  atomic writes:
    increase value incrementally by 1
  batch writes:
    many writes/updates at a time

dynamodb locking types:
  optimistic locking:
    prevents stale writes with no drop in performance
    each item has an attribute that acts as a version number and you can update item only if the version number on the server side has not changed
    implement with conditional writes:
      only updates if condition is true:
  pestimistic locking:
    prevents stale writes with significant drops in performance
  overly optimistic locking:
    prevents stale writes with a system that implements only one user

to avoid hot partitioning:
  implement dynamodb write sharding:
    if you have two partition key values that will be used over and over, add random suffixes to partition key to allow a better distribution of items evenly across partitions

dynamodb patterns with s3:
  large objects:
    store large objects inside s3, index each item inside a dynodb table that stores a reference to object
    client can query the metadata via an api on top of dynamodb or just query dynamodb directly if they have the partition key

dynamodb access control:
  web identity federation or cognito identity pools:
    each user gets temp aws credentials
  iam role with conditions:
    LeadingKeys:
      this condition allows a user to only access items that are associated with their partition key
    attributes:
      limits specific attributes the user can see
    

  

logs in RDS:
  by default error logs are published to cloudwatch
  you can also enable slow query logs and audit logs

Invoke api for lambda:
  three types for invocationType:
    RequestResponse:
      default, invokes the function synchronously
    Event:
      invoke the function asynchronously
    DryRun:
      validates parameter values and verifies that the user or role has permission to invoke fn

lambda authorizers:
  api gateway feature that allows a lambda function to control access to your api
  token based:
    receives the caller's identity in a bearer token
  request parameter-based:
    receives the caller's identity in combination of headers, query string params, and variables
cognito authorizers:
  api gateway feature that allows cognito to control access to your api
  implementation:
    create an user pool
    create authorizer in api gateway using the cognito user pool id
    set the name of the header that will be used from the request to the user pool as a token source for authorization


difference between 's3:PostObject' and 's3:PutObject':
  both upload an object to bucket, but different method of uploading
  post:
    uploads object via HTML form
  put:
    uploads object via pre-signed url or s3 endpoint

ecs task placement process:
  ecs identifies the instances that satisfy requirements in the task definition:
  ecs identifies the instances that saisfy the task placement constraints:
  ecs identifies the instances that saisfy the task placement strategies:
  ecs then selects the instances for task placement
ecs task placement constraints:
  distinctInstance constraint:
    place each task on a different container instance
  memberOf:
    places task on instances that satisfy an expression, using cluster query language
    cluster query language:
      expressions that enable you to group objects, like container instances by a specific attribute
ecs task placement strategies (for ec2 mode only):
  algorithms for selecting instances for task placement or tasks for termination:
  binpack:
    places tasks based on the least available amount of cpu or memory by filling a container before placing tasks on another instance:  
    minimizes the number of instances in use.
  random:
    places tasks randomly:
  spread:
    places tasks evenly across specified 'bins' given in the field attribute:
    ex...instanceId, ecs.availability-zone


cloudwatch detailed monitoring:
  feature of cloudwatch that allows you to monitor your resources more frequently and capture short-lived performance changes that may not be captured with basic monitoring.
  not enabled by default
  if enabled, allows you to capture data with a 1-minute frequency for EC2 instances, and a 1-second frequency for other AWS resources VS 5 minute frequencies for all resources

iam policy simulator:
  evaluates the policies that are present in an environment and returns whether the requested action would be allowed or denied
dry run:
  checks whether you have the required permissions for the action, without actually making the request
  --dry-run parameter in cli:
    some cli commands contain this option to simulate api calls
    UnauthorizedOperation:
      lets us know we don't have the access permissions to run command
    DryRunOperation:
      lets us know we do have the access permissions to run command


kinesis adapter:
  recommended way to consume streams from dynamodb for real time processing of data

amazon cognito sync:
  enables cross device syncing of application related user data for one user
  you can use it to synchronoize user profile data across mobile devices and the web without requiring your own backend

aws distro for opentelemetry:
  broader tracing solution that includes, xray and other tracing solutions
  ability to send traces to multiple different tracing backends simultaneously without having to reinstrument your code
  open source

cloudwatch metrics:
  latency:
    measures the overall responsiveness of your api calls
  integrationLatency:
    measures the responsiveness of the backend not the requests which are served from the backend
  CacheHitCount:
    fetches the number of requests served from cache
  CacheMissCount:
    fetches the number of requests served from a backend in a given period

aws amplify:
  set of purpose built tools and features that enables frontend web and mobile developers to build full stack apps on aws
  amplify hosting:
    proves git based workflow for hosting full stack serverless web apps w continuous deployment
    cypress tests:
      update build settings in the amplify.yml config file
  amplify studio:
    visual dev env that implements process where you build your frontend UI with a set of ready to use UI components, and create an app backend with aws resources 
  amplify libraries:
    connects your app to aws services

aws CodeStar:
  allows you to quickly develop, build, and deploy apps on aws
  project dashboard contains app activity
  integrated solution that contains:
    github, codecommit, codebuild, codedeploy, cloudformation, codepipeline, cloudwatch, jira, beanstalk, cloud9, lambda, and ec2

aws CDK, cloud development kit:
  open source software development framework to provision your cloud application resources using familiar programming languages (js, typescript, python, java, and .net)
  constructs:
    high level class libraries that allow you to provision resources

signing AWS API requests:
  all AWS API requests implement the HTTP api:
    SigV4 provides implementation for signing HTTP api requests
    if you use the SDK or CLI, these requests are signed for you

before a cloudformation template can be used by cloudformation, it must be uploaded to:
  s3, even if using the console it uploads to s3 under the hood and references that template

cloudwatch synthetics canary:
  configurable script that monitors your resources and reproduces what your customers do programmatically to find issues before customers are impacted

lambda and codedeploy:
  codedeploy can help you automate traffic shift for lambda aliases via the SAM framework
  
codeArtifact:
  artifact management system that helps you store and retrieve software dependencies
  implements common dependency management tools such as maven, gradle, npm, yarn, twine, pip, and nuget

amazon codeGuru:
  ml powered service that automates code reviews and application performance recommendations

amazon cloud9:
  cloud based integrated development environment
  code editer, debugger, and terminal in a browser
  prepacked with essential tools for popular programming languages

nitro enclaves:
  allows you to process highly sensite data in an isolated compute environment

to enforce ssl requests to objects stores inside s3 bucket:
  implement aws:SecureTransport iam action

Amazon OpenSearch Service:
  allows you to implement searches on any database for fields, or partial fields
  can also perform analytic queries from data ingested by firehose, iot, and cloudwatch logs
  amazon elasticsearch v2

Amazon Managed Streaming for Apache Kafka (Amazon MSK):
  is a fully managed apache kafka on aws
  provides an alternative to amazon kinesis to stream data
  mfk serverless:
    allows you to run msk without managing the capacity
  differences from kinesis:
    higher message size limit, >1 MB
    unlimited time for data storage

aws AppConfig:
  allows you to deploy dynamic configuration changes to your applications independently of any code deployments because the configurations are stored outside of the app in AppConfig











  
     