AWS SECURITY TOKEN SERVICE, STS:
  provides identities assuming a role the temporary security credentials they need for access:
  sts:AssumeRole action generates temporary credentials:
  credentials are similar to access keys:
  credentials can be given subset of permissions of permissions policy of role:
  when an identity assumes a role:
    sts:AssumeRole calls are made to STS
    STS requests permissions policy from role and generates credentials based on permissions policy
    STS returns credentials
  temporary credentials contain:
    AccessKeyId, Expiration, SecretAccessKey, SessionToken

  AssumeRoleWithWebIdentity:
    returns a set of temporary security credentials for federeated users who authenticated through a public identity provider, like amazon, google, etc
  AssumeRoleWithSAML:
    returns a set of temporary security credentials for federeated users who authenticated through your organization's identity system that implements SAML2.0, like AD
  GetSessionToken:
    returns a set of temporary security credentials for an aws account or iam user.  you use it if you want to require MFA to allow only certain devices to access specific aws api operations

  sts:DecodeAuthorizationMessage:
    allows a user to decode an encoded error message
  

-----------------------------------------------------------------------------------------------:
S3 BUCKET KEYS (know all):
  using S3 SEE-KMS without bucket keys:
    each object uploaded via PUT method uses a unique DEK when using KMS key
    the unique DEK is stored with the object
    limits scalability
  using S3 SEE-KMS with bucket keys:
    KMS key is used to generate a time limited bucket key
    time limited bucket key:
      generates DEKs within s3, offloading the work from KMS and reducing the number of KMS api calls
    increases scalability
    cloudtrail kms event logs now show the bucket ARN instead of object ARN
    works with replication...object encryption is maintained
    not retroactive

-----------------------------------------------------------------------------------------------:
CLOUDWATCH LOGS ARCHITECTURE:
  two sides:
    ingestion side:
      public service that allows you to store, monitor, and access logging data
      natively logs AWS services logs (VPC flow logs, cloudtrail, ebs, ecs, api gateway, lambda, route53, and others)
      CWAgent allows you to log system, custom application, and on premises logging
    subscription side:
      log events:
        a specific occurrence or action that is logged by a service or application
        consist of timestamp and raw message
      log stream:
        a collection of log events that are generated by a single source, such as an EC2 instance
      log group:
        a collection of related log streams
        sets retention (by default logs are stored indefinitely), access permissions, and encryption at rest using KMS
        defines metric filter to analyze patterns within log group and creates a metric
        subscription filter allows real time delivery
      s3 export:
        logs are exported to s3 bucket via CreateExportTask api call
        takes up to 12 hours
        you can encrypt data with only sse-s3 encryption method
      subscription filter:
        allows real time and near real time delivery of log groups
        configures pattern, destination ARN, distribution, access permissions
        near realtime delivery:
          kinesis data firehose allows near realtime delivery
        real time delivery:
          aws managed lambda function natively delivers into elasticsearch in realtime delivery
          custom lambda functions can be used to export data to nearly any destination in realtime
          can deliver to a kinesis data stream in realtime

-----------------------------------------------------------------------------------------------:
S3 REQUESTOR PAYS FEATURE:
  allows requestor of data, instead of bucket owner, to pay the cost of the request:
  unathenticated access is not supported:
  doesn't work with static website hosting or bitTorrent
  bucket level setting
  requesters must add x-amz-request-payer header to confirm payment responsibility

-----------------------------------------------------------------------------------------------:
POLICY INTERPRETATION:
  follow these steps when evaluating policy:
    identify number of statements:
    identify at a high level what each statement does:
    identify overlap of any statements:
    statements with condition field take effect when that condition is true
    'NotAction' field allows all actions besides the actions listed in value, there are a lot of inverses used in policies so be on lookout for those on exam

-----------------------------------------------------------------------------------------------:
POLICY EVALUATION LOGIC:
  for same account:
    aws checks pile of policies in this order:
      explicit denys:
        if it denies, permission is denied and evaluation stops
      organization SCPS on identity's account:
        if it denies, permission is denied and evaluation stops
      resource policies:
        if it allows, permission is allowed and evaluation stops
      IAM identity/permission boundaries:
        permission boundaries allow you to limit permissions granted
        if it denies, permission is denied and evaluation stops
      role/session policies:
        session policies allow you only a subset of role permissions
        if it exists and it denies, permission is denied and evaluation stops
      identity policies:
        access is denied or allowed, or implicit deny
    
    for multi account (account a trying to access account b's resources):
      has same checks as above for account a and also needs a resource policy allowing in account b 

-----------------------------------------------------------------------------------------------:
X-RAY:
  distributed tracing application, it is designed to track sessions through an application:
  aggregrates different services data to give you a single overview of flow of data:
  services sending data into xray require iam permissions:
  tracing header:
    the first service of session generates this which is used to track a request through your distributed application
  trace:
    collects all the segments generated by a single request
    maximum size is 500kb
    trace segment:
      json representation of trace
  segments:
    single block of data sent into xray that details requests within your application
    contains host/ip, request, response, work done (times), and issues
  subsegments:
    more granular version of segments, and provide additional details about a call to an aws service, an external HTTP API, or a sql database
    can instrument specific functions or lines of code in your application
    namespace field:
      set to "aws" for aws sdk calls and to "remote" for other downstream calls
    annotations:
      simple key value pairs that are indexed for use with filter expressions.  Record data that you want to use to group traces
  service graph:
    JSON document that details services and resources which make up your app
    retained for 30 days
  service map:
    visual version of the service graph 
  filter expressions:
    find traces related to specific paths or users
  metadata:
    more complex key value pairs with values of any type, but are not indexed.  Record data that you want to store in the trace but you don't need to use for searching traces
  x-ray daemon:
    collects segments for multiple requests and uploads them to xray in one api call
    lightweight process that runs on each EC2 instance in your application's environment
  x-ray agent:
     a software library that you can integrate into your application's code. 
  sampling:
    process of selecting and recording a subset of incoming requests for tracing and analysis.  samples a portion of the requests rather than tracing every request.
  to integrate with aws ray these services require configuration:
    ec2: xray agent, xray daemon
    ecs:
      xray agent in tasks
      create a docker image that runs the xray daemon, upload it to a docker repo, and deploy to your ecs cluster
      to allow communication with xxray daemon, allow traffic on UPD port 2000 in your task definition file
    lambda: there is an 'enable aws x-ray' checkbox, installs xray agent
    beanstalk: agent preinstalled
    api gateway: per stage option
    sns and sqs: there is an enable option
    alb: adds trace id to the request header before sending it to target group

  PutTraceSegments:
    you can send segment documents directly to xray by using the PutTraceSegments api
    if segments aren't natively supported by the service, you can implement custom inferred segments by including subsegments

  x-ray fetches the client ip address via:
    the 'X-forwarded-for' header of the request
  how to view full traces of app without implementing xray console:
    use the 'GetTraceSummaries' api to get the list of trace ids
  

-----------------------------------------------------------------------------------------------:
CI/CD USING AWS CODE: 5:54
  CI/CD stands for Continuous Integration and Continuous Delivery (or Deployment), which are two practices in software development that aim to streamline and automate the process of building, testing, and deploying software.
    Continuous Integration:
      practice where developers frequently merge their code changes into a shared repository, and then automated build and test processes are triggered to ensure that the code is functioning properly and that new changes are not breaking any existing functionality.
    Continuous Development:
      practice where code changes are automatically built, tested, and prepared for release to production environments.

  stages of ci/cd development pipeline:
    code:
      focuses on actual coding process, storage, version control
    build:
      focuses on combining src code, libraries, frameworks, and generates output
    test:
      tests code against expectations
    deploy:
      getting code out to code environments where it will run
    
    without aws:
      manually configure... github (code) -> jenkins (build+test) -> jenkins/other tooling (deploy)
      integration with aws has to be configured
    with aws:
      CodeCommit (code) -> CodeBuild (build+test) -> CodeDeploy (deploy)
      CodePipeline service orchestrates all of these services together
  
  CodePipeline:
    each pipeline has stages
    every pipeline at minimum has source stage which defines where source code is located within specific branch within specific repo
    buildspec.yml:
      collection of build commands and related configuration that CodeBuild implements to run a build
    appspec.yml|json:
      defines exactly how a deployment process proceeds that CodeDeploy implements

  CodeDeploy destinations:
    codeDeploy:
      can be deployed onto one or more ec2 instances using a deployment group
    elastic beanstalk or OpsWorks:
    cloudformation:
      can create/update stacks
    ecs or ecs (blue/green deployment model):
    service catalog or alexa skills kit:
    s3:
    
-----------------------------------------------------------------------------------------------:
CodeCommit:
  main feature is repositories
  authentication to access repo via CL:
    SSH key pair: 
      in your IAM security credentials (public key is in developer's iam user and private is in repo)
    https:
      git credential helper:
        requires an aws credential profile, which stores access key id and secret access key
        aws cli includes this helper by default
      generate https git credentials and store in git credential manager:
        you configure the git credentials manually

  authorization for repo:
    access is given via IAM identity policies
  event driven architecture:
    notifications:
      allow you to send notifications to SNS or chatbot targets when events occur
    trigger:
      allow you to trigger SNS or lambda function targets when events occur

-----------------------------------------------------------------------------------------------:
CodePipeline:
  continuous delivery tool that orchestrates CodeCommit, CodeBuild, and CodeDeploy to work together
  pipelines are built from STAGES
  stages can have sequential or parallel ACTIONS
  movement between stages can be automatic or require manual approval and implements topic
  artifacts:
    files or assets input to an action or output from an action
    input artifacts: s3 -> action
    output artifact: action -> s3
  event driven:
    all state changes -> event bridge

-----------------------------------------------------------------------------------------------:
CodeBuild:
  build as a service product
  fully managed service that allows you to pay for only the resources consumed during builds
  it is an alternative to having a jenkins server that would be running all the time
  used for builds and tests before deployment:
  uses docker for build environment and can be customized
  integrates with aws services
  architecture:
    gets source from github, codecommit, codepipeline, s3, and more
    buildspec.yml:
      collection of build commands and configs codebuild implements to customize build
      has to be in the root of source folder
      contains environment variables
      contains config for artifacts
      four main phases:
        install:
          install packages in build env, like frameworks and tooling
        pre_build:
          install dependencies and sign into things
        build:
          commands run during the build process
        post_build:
          package things up, push docker image, explicit notifications
    build environment:
      can be customized via buildspec.yml and docker images
    build projects:
      controlled via cli, sdk, and pipeline
  logs: -> s3 and cloudwatch logs
  metrics: -> cloudwatch
  events: -> eventbridge

  to build with a proxy server:
    implement codebuild in a private subnet and a proxy server in a public subnet
    implement ssl-bump, buildspec.yml with proxy element

-----------------------------------------------------------------------------------------------:
CodeDeploy:
  code deployment as a service
  alternative to Jenkins, Ansible, Chef, Puppet, Cloudformation, and more..
  deploys code and prebuilt applications, not resources:
  can deploy to EC2, on premise servers, lambda functions, and ECS:
  CodeDeploy agent:
    required to deploy code to on premise servers:
  Appspec.yml|json:
    manages deployments
    contains configuration and lifecycle event hooks
    config components:
      files:
        applies to ec2 instances and on premise servers
      permissions: 
        details permissions for files, directories, and folder in files section (applies to ec2 instances and on premise servers)
      resources:
        applies to ecs and lambda
    lifecycle event hooks:
      specifies the event hook for corresponding events, if there is not an event hook for the incoming event then nothing happens
      implementation details are different if triggered on ec2/on premises than if triggered on ecs/lambda
      ec2 / on prem hooks:
        ApplicationStop:
        DownloadBundle:
        beforeInstall:
        Install:
        AfterInstall:
        ApplicationStart:
        ValidateService:
          verifies that the deployment was successful or not
      lambda/ecs hooks:
        start:
        beforeAllowTraffic:
        AllowTraffic:
        AfterAllowTraffic:
        end:
    deployment configurations:
      in place (ec2/on prem):
        turns off all existing instances, latest version is installed, and instance is restarted
      (blue/green)...:
        canary (lambda/ecs):
          traffic is shifted into two increments
          immediately first increment of traffic shifted, then the last increment after time period
        linear (lambda/ecs):
          traffic is shifted in equal increments with an equal number of time between each increment
      all at once (lambda/ecs):
        all traffic is shifted all at once
    deployment groups:
      allows you to deploy an application revision to different sets of instances at different times:
      can apply to individual instances or an autoscaling group:
    
-----------------------------------------------------------------------------------------------:
SQS EXTENDED CLIENT LIBRARY:
  used when handling messages over SQS max(256KB)
  often used with Java
  implementation of SendMessage api call:
    sends large payloads to s3 and stores a link in message stored in SQS
  (Receive/Delete)Message api call:
    loads/deletes large s3 payload
  
-----------------------------------------------------------------------------------------------:
LAMBDA LAYERS:
  zip archives that allow runtimes, libraries, or other dependencies to be modularized so they can be reused between functions
  libraries are extracted in /opt folder
  types:
    aws layers, custom layers, ARN reference to layer

-----------------------------------------------------------------------------------------------:
LAMBDA CONTAINER IMAGES:
  creates lambda function that can integrate with existing container workflows
  allows you to package code in containers within lambda, and then is added to the ECR
  useful feature for many organizations that use containers and CI/CD processes built for containers
  Lambda Runtime API:
    include this package inside your container images to allow interaction with lambda and your container
  AWS Lambda Runtime Interface Emulator, RIE :
    lets you do local testing to test integration with containers and lambda without using lambda

-----------------------------------------------------------------------------------------------:
LAMBDA AND ALB INTEGRATION:
  architecture of client <-> ALB <-> LAMBDA function req/res:
    ALB translates HTTP/S into a lambda compatible event (JSON structure)
    lambda respons in JSON which is translated back into HTTP/S response
    single value headers... (http//catagram.io?&search=winkie):
      json includes "queryStringParameters" :{ "search" :"winkie"}
    multi-value headers... (http//catagram.io?&search=roffle&search=winkie):
      json includes "multiValueQueryStringParameters" :{ "search" :["winkie", "roffle"]}

-----------------------------------------------------------------------------------------------:
LAMBDA FUNCTION URLS:
  allow you to configure a HTTPS endpoint in front of your lambda function without having to configure additional services besides Lamda (like api gateway)
  can be publicly accessible or require iam authentication

-----------------------------------------------------------------------------------------------:
LAMBDA RESOURCE POLICIES:
  define who can invoke or manage your lambda function:
  you can allow or deny certain entities from interacting with your lambda function
  same account permissions:
    either a identity policy/role is required for an entity to access lambda OR a lambda resource policy is required that grants entity permission to access lambda 
  cross account permissions:
    both an identity policy/role is required for an entity to access lambda AND a lambda resource policy is required that grants entity permission to access lambda 
    
-----------------------------------------------------------------------------------------------:
LAMBDA CONTEXT OBJECT:
  object that is passed to handler in lambda function by the lambda service:
  provides methods and properties that provide information about the invocation, function, and execution environment:

-----------------------------------------------------------------------------------------------:
API GATEWAY  HTTP METHODS AND RESOURCES:
  invoke URL:
    contains gateway endpoint dns name, stage, and resource
  methods:
    like HTTP methods, but have different integrations in addition to HTTP..like Lambda, other aws services, mock, and vpc link
  
-----------------------------------------------------------------------------------------------:
API GATEWAY USAGE PLAN: (know all)
  allows you to specify who can access one ore more deployed api stages and methods
  allows you to configure throttling limits and quota limits that are enforced on customer api keys

-----------------------------------------------------------------------------------------------:
API GATEWAY INTEGRATIONS:
  phases of api gateway request/response:
  request phase: 
    client makes a request to api gateway:
    method request:
      defines everything about client's request to the method, (path, headers, parameters )
    integration request:
      handles proxying data to form integration can handle or passes straight through
  integration phase:
    integration receives data, then performs some work, then passes data back to api gateway
  response phase:
    where response is sent to client:
    integration response:
      handles proxying data to form client can handle or passes straight through(headers,status codes, bodies)
    method response:
      handles how communication is delivered back to client 

  different types of integrations:
    mock:
      used for testing, no backend development because everything is handled within api gateway
    http:
      backend http endpoint, requires mapping templates that transform data
    http proxy:
      pass through to http integration unmodified, return to the client unmodified (backend needs to use supported format)
    aws:
      lets an api expose aws service actions, requires mapping templates that transform data
    aws proxy (lambda):
      passes through to aws integration unmodified, returns to client unmodified
  
  mapping templates:
    implements the Velocity Template Language (VTL) to transform data
    common exam question is that this can translate data from SOAP API to REST API

-----------------------------------------------------------------------------------------------:
API Swagger and OpenAPI:
  api gateway can export to OAS and import from OAS:
    api swagger:
      OpenAPI v2
    OpenAPI specification, OAS, v3:
      API description format for RESTful APIs
      defines endpoints, routes, input and output parameters, and authentication methods
  
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK, EB:
  allows you to upload your application, and automatically handles the details of provisioning and managing infrastructure: 
  requires application changes:
  PAAS, platform as a service
  great for small dev teams
  implements cloudformation to provision resources:
    you can customize the infrastructure as much as you want to:
  platforms:
    languages eb supports
    built in languages, docker, and custom platforms:
      go, java se, tomcat, .net core, .net, node.js, php, python, ruby
      single container docker, multicontainer docker, preconfigured docker
      custom platforms via packer
  elastic beanstalk application:
    a container for everything related to an application:
  environments:
    sub containers of application that contain infrastructure and configuration for a specific app version:
    each environment is either a web server tier or a worker tier
    web server tier:
      designed to communicate with end users:
    worker tier:
      designed to process work from web server tier:
      manages a amazon sqs queue and run a daemon process on instances that reads from the queue for you:
      includes asg, ec2 instances, and iam role
    each environment has its own CNAME and a generic DNS name:
    CNAME swap:
      environments can swap their CNAMEs for use cases like green/blue testing:
  versions:
    points at a specific version of deployable code for an application:
    source bundle is stored on s3
  keep databases outside of elastic beanstalk:
    dbs in an ENV are lost if the env is deleted

-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK DEPLOYMENT POLICIES:
  implement how application versions are deployed to environments
  different types:
    all at once:
      deploy application to all ec2 instances at once, and there is outage until whole deployment is complete
      deploys new version to existing ec2 instances
    rolling deployments:
      deploy in rolling batches, there is drop in performance because one batch will always be deploying
      deploys new version to existing ec2 instances
    rolling with additional batch:
      deploys in rolling batches with an additional batch always so capacity is maintained during the process
      deploys new version to existing ec2 instances
    immutable:
      new instances are deployed with new version in a diff asg all at once, once new instances are healthy they replace the old instances in original asg
      deploys new ec2 instances rather than deploying new versions on existing ec2 instances
    traffic splitting (blue/green):
      immutable, but implements traffic split process so before you replace the old instances you can test with traffic
      deploys new ec2 instances rather than deploying new versions on existing ec2 instances

    use the eb cli to deploy:
      eb deploy 
      generated via a zip or war file in .elasticbeanstalk/config.yml in project folder
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK LIFECYCLE AND RDS:
  you can create an RDS instance within an EB environment:
    the instance is then linked to the EB environment
    you should only do this for really small scall dev and test projects
    environment properties:
      RDS_HOSTNAME, RDS_PORT, RDS_DB_NAME, RDS_USERNAME, RDS_PASSWORD
  you can create and RDS instance outside of EB:
    environment properties need to point at RDS instance
    data is outside of the EB env lifecycle
  
  how to decouple existing RDS instance within EB from EB environment:
    create an RDS snapshot
    enable "delete protection"
    create a new eb environment with the same app version
    ensure new environment can connect to the DB, (sg and env properties)
    swap environments (CNAME or DNS)
    remove sg rule from old environment
    terminate the old environment
    locate DELETE_FAILED stack, manually delete and select 'retain stuck resources'

-----------------------------------------------------------------------------------------------:
CUSTOMIZING VIA .EBEXTENSIONS:
  allow you to customize EB environments
  implements Cloudformation to implement changes
  create a .ebextensions folder inside application source bundle:
    add YAML or JSON ending in .config extension
    sections:
      option_settings:
        allows you to set options of resources
      resources:
        allows you to create new resources

-----------------------------------------------------------------------------------------------:
EB and HTTPS:
  you need to apply the SSL cert to the load balancer directly:
    via EB console
    via .ebextensions/securelistener-alb.config, and remember to configure security group

-----------------------------------------------------------------------------------------------:
EB ENVIRONMENT CLONING:
  allows you to create a new environment by cloning an existing one:
  copies all changes you've made to environment via ebs console, but not unmanaged changes you've made via cli or api:
  when copying RDS, the instance is copied but data is not
  different platform branches (languages eb supports) are not guaranteed to be compatible
  you can clone via eb console, API, or "eb clone environmentName"

-----------------------------------------------------------------------------------------------:
EB AND DOCKER:
  single container mode:
    implement when you want to only run one container on one docker host per environment
    uses ec2 with docker, not ECS
    referred to as docker in console UI
    you provide 1 of these:
      dockerfile:
        eb will build a docker image and use this to run a container
      Dockerrun.aws.json (version 1):
        configures what image to use, ports, volumes, and other attributes
      docker-compose.yml:
        if you use docker dompose
  multi-container mode:
    implements multiple containers running on one or more ec2 instances per environment
    implements an elb
    creates an ecs cluster:
      contains ec2 instances with containers 
    requires a dockerrun.aws.json (v2) file:
      in application source bundle and creates an environment
    requires images to be stored in a container registry such as ECR:

-----------------------------------------------------------------------------------------------:
PRACTICE TEST QUESTIONS:

to include lambda code within a cfn template:
  Zipfile (field inside AWS::Lambda::Function resource): 
    "code here"

if s3 bucket is timing out when trying to list items:
  implement max-items parameter or page-size parameter to reduce number of objects returned
to implement more efficient reads on large dynamodb tables:
  reduce page size and implement query operations   
if cli script to list items is timing out:
  add pagination parameters in the aws cli command

if you are getting 403 error when trying to upload large file with kms key to s3 bucket:
  check to ensure you have the kms:Decrypt and kms:GenerateDataKey* actions because these permissions are required when s3 must decrypt and read data to complete the multipart upload

KCL, Kinesis Client Library:
  ensures there is a record processor to process each shard
  also tracks shards in the stream using a dynamodb table
  ec2 instances contain a record processer and one KCL worker
  number of instances should not exceed the number of shards, except for failure standy purposes:
  number of instances can be less than number of shards because one worker can process multiple shards:

how to give access to server in an on premise environment trying to access aws services:
  create a new IAM user with programmatic access via access keys
  create the credential file at ~/.aws/credentials
  it is always best practice to use role with ec2 instance, however when server is on prem then it can only access aws services via access keys

projection expression:
  allows you to get just some of dynamodb attributes rather than all of them when reading data from table
filter expressions:
  allows you to get just some of dynamodb items within the results returned to you when reading data from table
error retries and exponential backoff:
  uses progressively longer waits between retries for consecutive error responses

AWS SAM, serverless application model:
  toolkit that improves the developer experience of building and running serverless applications on aws
  benefits:
    defines your application infrastructure code quickly, using less code
    manage applications through their entire dev lifecycle
    quickly provision permissions between resources with sam connectors
    manage your terraform serverless applications
  to allow cloudformation to use SAM syntax in template:
    include Transform section:
      specifies the verions to use
  architecture:
    sam template specification:
      framework that you can use to define your serverless application infrastructure on AWS
      you can implement both the cloudformation and sam syntax within the same template
      implements cloudformation
    sam cli:
      command line tool used with sam templates and third party integrations such as terraform to build and run your serverless apps
      sam init:
        initilizes a serverless application with an aws sam template
      sam build:
        builds any dependencies that your application has
      sam package:
        this command zips your code artifacts, uploads them to s3, and produces a packaged aws sam template file that's ready to be used
      sam deploy:
        this command uses the sam package file to deploy your application

errors you might see when creating lambda functions via cli with CreateFunction API:
  InvalidParameterValueException:
    one of the parameters in the request is invalid
  ServiceException:
    the lambda service encountered an internal error

EventBridge rules and dynamodb:
  rules are not capable of detecting table level events from dynamodb

encrypt api:
  encrypts plaintext into ciphertext by using a CMK
generateDataKey api:
  returns a plaintext copy of the data key along with a copy of the encrypted data key:
  useful when you need to encrypt data immediately:
generateDataKeyWithoutPlaintext api:
  returns only a copy of the encrypted data key:
  useful when you need to encrypt data at some point, but not immediately:
decrypt api:
  decrypts the data key and returns a plaintext of the data copy key
overall process:
  decrypt data key into plaintext key:
  decrypt/encrypt data with plaintext key and discard plaintext key:
  store encrypted data key alongside the locally encrypted data:
top level plaintext key:
  master key:

cache-control: max-age=0 header:
  allows you to bypass cache and retrieve data from source:
if you want to only allow authorized clients to invalidate cache:
  select the 'require authorization' checkbox in the cache settings of your api via the console:
  execute-api:InvalidateCache action:
    allows iam role to make authorized invalide cache requests to the api
aws_iam method authorization type for api:
  allows you to limit the api to only be accessed by iam users or roles

lambda concurrent executions:
  refers to the number of executions of your function code that is happening at same time
  push based event sources:
    concurrent executions = (invocations per second) x (average execution duration in seconds)
    by default the limit per region is 1000 concurrent executions
    aws enforces the unreserved concurrency pool will always be at least 100 concurrent executions 
  poll based event sources:
    concurrent executions = at most the number of shards in a kinesis data stream

https status code errors in api gateway:
  504:
    integration failure, integration timeout, the integration timeout lasts from 50ms to 29 seconds for all integration types
  502 or 429:
    too many requests and api gateway is throttling

dynamodb errors:
  ProvisionedThroughputExceededException:
    not enough RCU or WCU left for operation
  ThrottlingException:
    rate of requests exceeds the allowed throughput
  RequestLimitExceeded:
    provisioned RCU or WCU (throughput) exceeds current throughput limit for account

dynamodb operations:
  BatchGetItem:
    allows you to get multiple items at once in parallel
  BatchWriteItem:
    allows you to write multiple items at once in parallel
  UpdateItem:
    allows you to perform write requests
    you can implement an atomic counter
  TransactWriteItems or TransactGetItems:
    allows you to group multiple actions together and submit them as a single all-or-nothing operation

  for each of these operations you can include 'ReturnConsumedCapacity' argument:
    to return the number of wcu consumed by the operation
    additional arguments:
      total:
        returns the total number of wcus consumed
      indexes:
        returns the total number of wcus consumed, with subtotals for the table and any secondary indexes that were affected by operation
      none:
        no wcu details are returned (default)
  

dynamodb locking types:
  optimistic locking:
    prevents stale writes with no drop in performance
    each item has an attribute that acts as a version number and you can update item only if the version number on the server side has not changed
    implement with conditional writes:
      only updates if condition is true:
  pestimistic locking:
    prevents stale writes with significant drops in performance
  overly optimistic locking:
    prevents stale writes with a system that implements only one user
  

logs in RDS:
  by default error logs are published to cloudwatch
  you can also enable slow query logs and audit logs

invoke api for lambda:
  three types for invocationType:
    RequestResponse:
      default, invokes the function synchronously
    Event:
      invoke the function asynchronously
    DryRun:
      validates parameter values and verifies that the user or role has permission to invoke fn

lambda authorizers:
  api gateway feature that allows a lambda function to control access to your api
  token based:
    receives the caller's identity in a bearer token
  request parameter-based:
    receives the caller's identity in combination of headers, query string params, and variables
cognito authorizers:
  api gateway feature that allows cognito to control access to your api
  implementation:
    create an user pool
    create authorizer in api gateway using the cognito user pool id
    set the name of the header that will be used from the request to the user pool as a token source for authorization


env.yaml:
  environment configurations for elastic beanstalk
cron.yaml:
  you can define periodic tasks and add to source bundle to add jobs to your worker's environment's queue automatically

lambda and xray environment variables:
  _X_AMZN_TRACE_ID:
    contains tracing header
  AWS_XRAY_CONTEXT_MISSING:
    contains behavior if function tries to record x-ray data but a tracing header is not available
  AWS_XRAY_DAEMON_ADDRESS:
    contains x-ray daemon's address and allows direct communication with the daemon vs implementing sdk


difference between 's3:PostObject' and 's3:PutObject':
  both upload an object to bucket, but different method of uploading
  post:
    uploads object via HTML form
  put:
    uploads object via pre-signed url or s3 endpoint

ecs task placement strategies:
  algorithms for selecting instances for task placement or tasks for termination:
  these are for ecs ec2 mode, fargate spreads out instances across azs by default:
  binpack:
    places tasks based on the least available amount of cpu or memory.  minimizes the number of instances in use.
  random:
    places tasks randomly, but still guarantees tasks are scheduled on instances with enough resources to run them
  spread:
    places tasks evenly across specified 'bins' given in the field attribute
cluster query language:
  expressions that enable you to group objects, like container instances by a specific attribute


cloudwatch detailed monitoring:
  feature of cloudwatch that allows you to monitor your resources more frequently and capture short-lived performance changes that may not be captured with basic monitoring.
  not enabled by default
  if enabled, allows you to capture data with a 1-minute frequency for EC2 instances, and a 1-second frequency for other AWS resources VS 5 minute frequencies for all resources

iam policy simulator:
  evaluates the policies that are present in an environment and returns whether the requested action would be allowed or denied
--dry-run parameter in cli:
  checks whether you have the required permissions for the action, without actually making the request

kinesis adapter:
  recommended way to consume streams from dynamodb for real time processing of data

amazon cognito sync:
  enables cross device syncing of application related user data for one user
  you can use it to synchronoize user profile data across mobile devices and the web without requiring your own backend

aws distro for opentelemetry:
  broader tracing solution that includes, xray and other tracing solutions
  ability to send traces to multiple different tracing backends without having to reinstrument your code

cloudwatch metrics:
  latency:
    measures the overall responsiveness of your api calls
  integrationLatency:
    measures the responsiveness of the backend not the requests which are served from the backend
  CacheHitCount:
    fetches the number of requests served from cache
  CacheMissCount:
    fetches the number of requests served from a backend in a given period

aws amplify:
  set of purpose built tools and features that enables frontend web and mobile developers to build full stack apps on aws
  amplify hosting:
    proves git based workflow for hosting full stack serverless web apps w continuous deployment
    cypress tests:
      update build settings in the amplify.yml config file
  amplify studio:
    visual dev env that implements process where you build your frontend UI with a set of ready to use UI components, and create an app backend with aws resources 

aws CodeStar:
  allows you to quickly develop, build, and deploy apps on aws
  project dashboard contains app activity such as recent code commits, builds, and deployments
  integrates with jira, beanstalk, lambda, and ec2

aws CDK, cloud development kit:
  open source software development framework to provision your cloud application resources using familiar programming languages
  constructs:
    high level class libraries that allow you to provision resources
  













  
     