AWS SECURITY TOKEN SERVICE, STS:
  provides identities assuming a role the temporary security credentials they need for access:
  sts:AssumeRole action generates temporary credentials:
  credentials are similar to access keys:
  credentials can be given subset of permissions of permissions policy of role
  when an identity assumes a role:
    sts:AssumeRole calls are made to STS
    STS requests permissions policy from role and generates credentials based on permissions policy
    STS returns credentials
  temporary credentials contain:
    AccessKeyId, Expiration, SecretAccessKey, SessionToken

  AssumeRoleWithWebIdentity api call:
    returns a set of temporary security credentials for federated users who authenticated through a public identity provider, like amazon, google, etc:
  AssumeRoleWithSAML api call:
    returns a set of temporary security credentials for federated users who authenticated through your organization's identity system that implements SAML2.0, like AD:
  GetSessionToken api call:
    returns a set of temporary security credentials for an aws account or iam user and implemented if you want to require MFA to allow only certain devices to access specific aws api operations:
    aws sts get-session-token [options]:

  sts:DecodeAuthorizationMessage:
    allows a user to decode an encoded error message:
    aws sts decode-authorization-message --encoded-message 'message':

-----------------------------------------------------------------------------------------------:
KMS ENCRYPTION PATTERNS AND ENVELOPE ENCRYPTION (KNOW ALL):
  when data is < 4kb in size:
    encrypt api:
      encrypts plaintext into ciphertext by implementing CMK in KMS
    decrypt api:
      decrypts ciphertext into plaintext by implementing CMK in KMS

  envelope encryption:
    implement when data is greater than 4kb in size, when you need to implement DEK
    process to encrypt data:
      generateDataKey api:
        returns a plaintext copy of the data key along with a copy of the encrypted data key
        the plaintext key is used to encrypt data, is discarded, and then encrypted data is stored with encrypted data key
        useful when you need to encrypt data immediately
      generateDataKeyWithoutPlaintext api:
        returns only a copy of the encrypted data key
        useful when you need to encrypt data at some point, but not immediately
        decrypt api:
          call later when you want to decrypt the data key and return a plaintext copy of the data key
    process to decrypt data:
      decrypt api:
        returns a plaintext copy of the encrypted data key stored with encrypted data
        plaintext key is used to decrypt data and encrypted data key and plaintext copy are discarded 
    top level plaintext key:
      master key
    data key caching:
      allows you to re-use data keys instead of creating new ones for each encryption
      the aws Encryption Key SDK implements data key caching

  s3 bucket key:
    kms creates a s3 bucket key and that s3 bucket key is used to create new data keys used for encrypting s3 objects INSTEAD of calling KMS every time to create new data key:
    implemented with sse-kms encryption:
    reduces number of api calls made to kms by 99%
    cloudtrail kms event logs now show the bucket ARN instead of object ARN

-----------------------------------------------------------------------------------------------:
CLOUDWATCH LOGS ARCHITECTURE:
  two sides:
    ingestion side:
      public service that allows you to store, monitor, and access logging data
      natively logs AWS services logs (VPC flow logs, cloudtrail, ebs, ecs, api gateway, lambda, route53, and others)
      CWAgent allows you to log system, custom application, and on premises logging
    subscription side:
      log group:
        a collection of related log streams that are derived from different sources:
        ex: /aws/lambda/WebAppServers:
        defines metric filter that analyzes patterns within log groups:
        defines subscription filter that allows real time delivery from cw to other services:
        sets retention (by default logs are stored indefinitely), access permissions, and encryption at rest using KMS
      log stream:
        a collection of log events that are generated by a single source, like an ec2 instance:
        ex: Log Stream: WebAppServer-1:
      log events:
        consist of timestamp and raw message:
        a specific action that is logged by a service or application
        ex: Timestamp: 2022-05-10T09:15:23Z,  Message: Page not found - 404 :
      s3 export feature:
        logs are exported to s3 bucket via CreateExportTask api call:
        takes up to 12 hours
        you can encrypt data with only sse-s3 encryption method
      subscription filter:
        allows real time and near real time delivery of log groups from cw to other services:
        configures pattern, destination ARN, distribution, access permissions
        near realtime delivery:
          implemented via kinesis data firehose:
        real time delivery:
          implemented via kinesis data streams, lambda functions, and aws managed lambdas and elasticsearch:
          aws managed lambda function natively delivers into elasticsearch in realtime delivery
          custom lambda functions can be used to export data to nearly any destination in realtime
          can deliver to a kinesis data stream in realtime
-----------------------------------------------------------------------------------------------:
S3 REQUESTOR PAYS FEATURE:
  allows requestor of data, instead of bucket owner, to pay the cost of the request:
  unathenticated access is not supported:
  doesn't work with static website hosting or bitTorrent
  bucket level setting
  requesters must add x-amz-request-payer header to confirm payment responsibility

-----------------------------------------------------------------------------------------------:
POLICY INTERPRETATION:
  follow these steps when evaluating policy:
    identify number of statements:
    identify at a high level what each statement does:
    identify overlap of any statements:
    statements with condition field take effect when that condition is true
    'NotAction' field allows all actions besides the actions listed in value, there are a lot of inverses used in policies so be on lookout for those on exam

-----------------------------------------------------------------------------------------------:
POLICY EVALUATION LOGIC:
  for same account:
    aws checks pile of policies in this order:
      each check not highlighted checks for denials and proceeds if no denial:
      explicit denys:
        if it denies, permission is denied and evaluation stops
      organization SCPS on identity's account:
        if it denies, permission is denied and evaluation stops
      resource policies:
        if it allows, permission is allowed and evaluation stops:
      IAM identity/permission boundaries:
        permission boundaries allow you to limit permissions granted
        if it denies, permission is denied and evaluation stops
      role/session policies:
        session policies allow you only a subset of role permissions
        if it exists and it denies, permission is denied and evaluation stops
      identity policies:
        access is denied or allowed, or implicit deny:
    
    for multi account (account a trying to access account b's resources):
      has same checks as above for account a and also needs a resource policy allowing in account b 

-----------------------------------------------------------------------------------------------:
AWS CLI CREDENTIALS PROVIDER CHAIN:
  cli will look for credentials in this order:
    1. command line options:
    2. environment variables:
    3. cli credentials file:
    4. cli configuration file:
    5. container credentials:
    6. instance profile credentials:

-----------------------------------------------------------------------------------------------:
tracing: 
  records a request from end to end:
instrumentation:
  to enable tracing:
  measure of a product's performance, diagnose errors, and to write trace information
X-RAY:
  distributed tracing application, it is designed to track sessions through an application:
  aggregrates different services data to give you a single overview of flow of data:
  trace:
    composed of all the segments generated by a single request:
    maximum size is 500kb
    trace segment:
      json representation of trace
    tracing header:
      used to track a request through your distributed application:
      the first service of session generates the tracing header: 
  segments:
    single block of data sent into xray that details requests within your application:
    contains host/ip, request, response, work done (times), and issues:
  subsegments:
    more granular version of segments:
    provide additional details about a call to an aws service, an external HTTP API, or a sql database:
    can instrument specific functions or lines of code in your application:
    namespace field:
      set to "aws" for aws sdk calls and to "remote" for other downstream calls
    annotations:
      simple key value pairs that index traces and are implemented via filter expressions:
      used to group traces
      filter expressions:
        find traces related to specific paths or users:
  service graph:
    JSON document that details services and resources which make up your app:
    retained for 30 days:
  service map:
    visual version of the service graph 
  metadata:
    more complex key value pairs with values of any type:
    they are not indexed:
    they store data in the trace:
  x-ray daemon:
    collects segments for multiple requests and uploads them to xray in one api call:
    lightweight process that runs on each EC2 instance in your application's environment:
  x-ray agent:
     a x-ray daemon that works in conjunction with x-ray sdk to instrument code and send to xray:
  sampling:
    records a portion of the requests rather than tracing every request:
    reduces cost and amount of data you record
  integrates with (know all):
    ec2 instances and on premise servers
    lambda
    ecs
    api gateway
    beanstalk
    elb, sns, sqs
  how to configure?:
    your code must import the x-ray sdk:
    you must install the x-ray agent manually or enable x-ray aws integration for some services to run the xray agent for you:
  xray troubleshooting:
    services sending data into xray require iam write permissions:
    you should ensure xray daemon is running:
  to integrate with aws xray these services require configuration:
    ec2: 
      xray sdk needs to be imported to run application code:
      xray agent needs to be installed:
      needs to have instance role with proper access:
    lambda: 
      there is an 'enable active tracing' checkbox, that installs the xray agent:
      make sure your application code is instrumented with the xray sdk:
      include 'AWSXRayDaemonWriteAccess' managed policy in execution role:
    ecs:
      define xray agent in tasks:
      create a docker image that runs the xray agent, upload it to a docker repo, and deploy to your ecs cluster:
      to allow communication with xray agent, allow traffic on UDP port 2000 in your task definition file:
    beanstalk:
      xray agent can be installed by setting an option in eb console or with a config file in .ebextensions/xray-daemon.config:
      application code is instrumented with the xray sdk: 
      ec2 instance has role with proper access:
      xray daemon is not provided for multicontainer docker, you need to configure yourself
    api gateway: 
      per stage option
    sns and sqs: 
      there is an enable option
    alb: 
      adds trace id to the request header before sending it to target group:

  xray APIs:
    writes:
      PutTraceSegments:
        allows you to send segment documents directly to xray:
        if segments aren't natively supported by the service, you can implement custom inferred segments by including subsegments
      PutTelemetryRecords: 
        used by the aws xray daemon to upload telemetry records, or aggegrated stats:
      GetSamplingRules:
        gets all of the sampling rules, and writes updates to the daemons:
    reads:
      GetTraceSummaries:
        retrieves a list of trace summaries that match the specified filter conditions.:
        includes basic info about the trace:
      BatchGetTraces:
        retrieves the complete trace data specified by id:
        includes the segment documents that contain detailed timing information and metadata for individual segments within the trace:
      GetTraceGraph:
        retrieves a service graph for one or more specific trace ids
      GetServiceGraph:
        gets service graph

  lambda and xray environment variables:
    _X_AMZN_TRACE_ID:
      contains tracing header:
    AWS_XRAY_CONTEXT_MISSING:
      contains behavior if function tries to record x-ray data but a tracing header is not available
    AWS_XRAY_DAEMON_ADDRESS:
      contains x-ray daemon's address and allows direct communication with the daemon vs implementing sdk:
  

-----------------------------------------------------------------------------------------------:
CI/CD USING AWS CODE:
  CI/CD stands for Continuous Integration and Continuous Delivery (or Deployment), which are two practices in software development that aim to streamline and automate the process of building, testing, and deploying software.
    Continuous Integration:
      practice where developers frequently merge their code changes into a shared repository: and then automated build and test processes are triggered to ensure that the code is functioning properly and that new changes are not breaking any existing functionality
    Continuous Development:
      practice where code changes are automatically built, tested, and prepared for release to production environments.:

  stages of ci/cd development pipeline:
    code:
      focuses on actual coding process, storage, version control
    build:
      focuses on combining src code, libraries, frameworks, and generates output
    test:
      tests code against expectations
    deploy:
      getting code out to code environments where it will run
    
    without aws:
      manually configure... github (code) -> jenkins (build+test) -> jenkins/other tooling (deploy)
      integration with aws has to be configured
    with aws:
      CodeCommit (code) -> CodeBuild (build+test) -> CodeDeploy (deploy):
      CodePipeline service orchestrates all of these services together
  
  CodePipeline:
    each pipeline has stages
    every pipeline at minimum has source stage which defines where source code is located within specific branch within specific repo
    buildspec.yml:
      collection of build commands and related configuration that CodeBuild implements to run a build
    appspec.yml|json:
      defines exactly how a deployment process proceeds that CodeDeploy implements

  CodeDeploy destinations:
    codeDeploy:
      can be deployed onto one or more ec2 instances using a deployment group
    elastic beanstalk or OpsWorks:
    cloudformation:
      can create/update stacks
    ecs or ecs (blue/green deployment model):
    service catalog or alexa skills kit:
    s3:
    
-----------------------------------------------------------------------------------------------:
CodePipeline:
  continuous delivery tool that orchestrates CodeCommit, CodeBuild, and CodeDeploy to work together:
  pipelines are built from STAGES:
  stages can have sequential or parallel ACTIONS:
  movement between stages can be automatic or require manual approval and implements sns topic:
  artifacts:
    files or assets input to an action or output from an action
    input artifacts: s3 -> action
    output artifact: action -> s3
  event driven:
    all state changes -> event bridge
-----------------------------------------------------------------------------------------------:
CodeCommit:
  main feature is repositories:
  authentication to access repo via CL:
    SSH key pair: 
      in your IAM security credentials (public key is in developer's iam user and private is in repo):
    https:
      git cli credential helper:
        requires an aws credential profile, which stores access key id and secret access key:
        aws cli includes this helper by default
      git credential manager:
        you configure the git credentials manually and store in git credential manager:

  authorization for repo:
    access is given via IAM identity policies:
  event driven architecture:
    notifications:
      allow you to send notifications to SNS or chatbot targets when events occur
    triggers:
      allow you to trigger SNS or lambda function targets when events occur:

-----------------------------------------------------------------------------------------------:
CodeBuild:
  build as a service product
  fully managed service that allows you to pay for only the resources consumed during builds:
  used for builds and tests before deployment:
  uses docker for build environment and can be customized:
  it is an alternative to having a jenkins server that would be running all the time
  integrates with aws services
  architecture:
    can get source code from github, codecommit, codepipeline, s3, and more
    buildspec.yml:
      collection of build commands and configs codebuild implements to customize build:
      has to be in the root of source folder:
      contains environment variables:
        can reference parameter store parameters or secrets manager secrets:
      to automatically encrypt build artifacts:
        specify a kms key:
      contains config for artifacts, you can enable dependencies caching in s3:
      four main phases (read all):
        install:
          install packages in build env, like frameworks and tooling
        pre_build:
          install dependencies and sign into things
        build:
          commands run during the build process
        post_build:
          package things up, push docker image, explicit notifications
    build environment:
      can be customized via buildspec.yml and docker images
    build projects:
      controlled via cli, sdk, and pipeline
  logs: -> s3 and cloudwatch logs
  metrics: -> cloudwatch
  events: -> eventbridge

  to build with a proxy server:
    implement ssl-bump, buildspec.yml with proxy element:
    implement codebuild in a private subnet and a proxy server in a public subnet

-----------------------------------------------------------------------------------------------:
CodeDeploy:
  code deployment as a service
  alternative to Jenkins, Ansible, Chef, Puppet, Cloudformation, and more..
  fully managed service that allows you to deploy code and prebuilt applications, not resources:
  can deploy to EC2, on premise servers, lambda functions, and ECS:
  implements rollbacks and can trigger cloudwatch alarms:
  CodeDeploy agent:
    required to deploy code to on premise servers:
  Appspec.yml|json:
    implements how deployments will happen:
    contains configuration and lifecycle event hooks:
    config components:
      files:
        applies to ec2 instances and on premise servers
      permissions: 
        details permissions for files, directories, and folder in files section (applies to ec2 instances and on premise servers)
      resources:
        applies to ecs and lambda
    lifecycle event hooks:
      defines custom functions (hooks) to run for corresponding events (stages of deployment process), if there is not an event hook for the incoming event then nothing happens when that stage of deployment lifecycle occurs:
      implementation details are different if triggered on ec2/on premises than if triggered on ecs/lambda
      ec2 / on prem hooks (ADBIAAV):
        ApplicationStop:
        DownloadBundle:
        beforeInstall:
        Install:
        AfterInstall:
        ApplicationStart:
        ValidateService:
          verifies that the deployment was successful or not:
      lambda/ecs hooks:
        start:
        beforeAllowTraffic:
        AllowTraffic:
        AfterAllowTraffic:
        end:
    deployment policies (know all):
      (ec2/on prem):
        in place:
          turns off all existing instances, latest version is installed, and instance is restarted
        blue/green:
          manually create a completely new environment and switch cname for alb:
          AllAtOnce: most downtime
          HalfAtATime: reduced capacity by 50%
          OneAtATime: slowest, lowest availability impact
          custom: define your %
      (lambda/ecs):  
        traffic shift (blue/green):
          canary:
            traffic is shifted into two increments
            immediately first increment of traffic shifted, then the last increment after time period
          linear:
            traffic is shifted in equal increments with an equal number of time between each increment
          all at once:
            all traffic is shifted all at once
    deployment groups:
      allows you to deploy an application revision to different sets of instances at different times:
      can apply to individual instances or an autoscaling group:
    
-----------------------------------------------------------------------------------------------:
SQS EXTENDED CLIENT LIBRARY:
  used when handling messages over SQS max size(256KB):
  implements Java library:
  implementation of SendMessage api call:
    sends large payloads to s3 and stores a pointer link in message stored in SQS
  (Receive/Delete)Message api call:
    loads/deletes large s3 payload
  
-----------------------------------------------------------------------------------------------:
LAMBDA LAYERS:
  zip archives that allow runtimes, libraries, or other dependencies to be modularized so they can be reused between functions:
  libraries are extracted from the layers into the /opt folder:
  types:
    aws layers, custom layers, ARN reference to layer

-----------------------------------------------------------------------------------------------:
LAMBDA CONTAINER IMAGES:
  creates lambda function that can integrate with existing container workflows:
  allows you to package code in containers within lambda, and then is added to the ECR:
  useful feature for many organizations that use containers and CI/CD processes built for containers
  Lambda Runtime API:
    include this package inside your container images to allow interaction with lambda and your container:
  AWS Lambda Runtime Interface Emulator, RIE :
    lets you do local testing to test integration with containers and lambda without using lambda:

-----------------------------------------------------------------------------------------------:
LAMBDA AND ALB INTEGRATION:
  architecture of client <-> ALB <-> LAMBDA function req/res:
    ALB translates HTTP/S into a lambda compatible event (JSON structure):
    lambda sends response in JSON which is translated back into HTTP/S response:
    
    single value headers... (http//catagram.io?&search=winkie):
      json includes "queryStringParameters" :{ "search" :"winkie"}
    multi-value headers... (http//catagram.io?&search=roffle&search=winkie):
      json includes "multiValueQueryStringParameters" :{ "search" :["winkie", "roffle"]}

-----------------------------------------------------------------------------------------------:
LAMBDA FUNCTION URLS:
  allow you to configure a HTTPS endpoint in front of your lambda function without having to configure additional services besides Lambda (like api gateway)
  can be publicly accessible or require iam authentication

-----------------------------------------------------------------------------------------------:
LAMBDA RESOURCE POLICIES:
  define who can invoke or manage your lambda function
  you can allow or deny certain entities from interacting with your lambda function
  same account permissions:
    either a identity policy/role is required for an entity to access lambda OR a lambda resource policy is required that grants entity permission to access lambda 
  cross account permissions:
    both an identity policy/role is required for an entity to access lambda AND a lambda resource policy is required that grants entity permission to access lambda 
    
-----------------------------------------------------------------------------------------------:
two main objects are passed as parameters to the Lambda function handler:
  context object:
    provides methods and properties that provide information about the invocation, function, and execution environment that allow lambda fn to interact with lambda service:
  event object:
    The event object represents the input data or the triggering event that invokes the Lambda function. It contains information specific to the event source that triggered the function.:

-----------------------------------------------------------------------------------------------:
API GATEWAY HTTP METHODS AND RESOURCES:
  invoke URL:
    contains gateway endpoint dns name, stage, and resource:
    https://{api-id}.execute-api.{region}.amazonaws.com/{stage}/{resource-path}:
  methods:
    like HTTP methods, but have different integrations in addition to HTTP..like Lambda, other aws services, mock, and vpc link
  
-----------------------------------------------------------------------------------------------:
API GATEWAY USAGE PLAN: (know all)
  allows you to specify who can access one ore more deployed api stages and methods
  allows you to configure throttling limits and quota limits that are enforced on customer api keys

-----------------------------------------------------------------------------------------------:
API GATEWAY INTEGRATIONS(KNOW ALL):
  phases of api gateway request/response:
  request phase: 
    client makes a request to api gateway
    method request:
      defines everything about client's request to the method, (path, headers, parameters )
    integration request:
      handles proxying data to form integration can handle or passes straight through
  
  integration phase:
    integration receives data, then performs some work, then passes data back to api gateway
  
  response phase:
    where response is sent to client
    integration response:
      handles proxying data to form client can handle or passes straight through(headers,status codes, bodies)
    method response:
      handles how communication is delivered back to client 

  different types of integrations:
    mock:
      used for testing, no backend development because everything is handled within api gateway
    http:
      backend http endpoint, requires mapping templates that transform data
    http proxy:
      pass through to http integration unmodified, return to the client unmodified
    aws:
      lets an api expose aws service actions, requires mapping templates that transform data
    aws proxy:
      passes through to aws integration unmodified, returns to client unmodified
  
  mapping templates:
    implements the Velocity Template Language (VTL) to transform data
    common exam question is that this can translate data from SOAP API to REST API

-----------------------------------------------------------------------------------------------:
API Swagger and OpenAPI:
  api gateway can export to OAS and import from OAS:
    api swagger:
      OpenAPI v2
    OpenAPI specification, OAS, v3:
      API description format for RESTful APIs:
      defines endpoints, routes, input and output parameters, and authentication methods:
  
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK, EB (KNOW ALL):
  allows you to upload your application, and automatically handles the details of provisioning and managing infrastructure 
  requires application changes
  PAAS, platform as a service
  implements cloudformation to provision resources:
    you can customize the infrastructure as much as you want to
  platforms:
    are languages eb supports
    built in platforms, docker, and custom platforms include:
      go, java se, tomcat, .net core, .net, node.js, php, python, ruby
      single container docker, multicontainer docker, preconfigured docker
      custom platforms via packer
  elastic beanstalk application:
    a container for everything related to an application
    environments:
      sub containers of application that contains a specific app version
      each environment has its own CNAME and a generic DNS name
      each environment is either a web server tier or a worker tier:
        web server tier:
          designed to communicate with end users
        worker tier:
          designed to process work from web server tier
          manages an amazon sqs queue and runs a daemon process on instances that reads from the queue for you
          includes asg, ec2 instances, and iam role
    CNAME swap:
      environments can swap their CNAMEs for use cases like blue/green testing
  versions:
    snapshot of deployable code for an application
    contains configurations and resources
    source bundle is stored on s3
  good practice is to keep databases outside of elastic beanstalk:
    dbs in an ENV are lost if the env is deleted

-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK DEPLOYMENT POLICIES (KNOW ALL):
  implement how application versions are deployed to environments
  different types:
    all at once:
      deploy application to all ec2 instances at once, and there is outage until whole deployment is complete
      deploys new version to existing ec2 instances
    rolling deployments:
      deploy in rolling batches, there is drop in performance because one batch will always be deploying
      deploys new version to existing ec2 instances
    rolling with additional batch:
      deploys in rolling batches with an additional batch always so capacity is maintained during the process
      deploys new version to existing ec2 instances
    immutable:
      new instances are deployed with new version in a diff asg all at once, once new instances are healthy they replace the old instances in original asg
    traffic splitting:
      canary testing.. identical to immutable but before you replace old instances with new you can test a % of traffic to new deployment
    blue/green:
      not a direct feature of eb
      manually create a completely new environment and switch over when ready by swapping urls
      route 53 can be setup using a weighted policy to test new environment before swapping

    use the eb cli to deploy:
      eb deploy 
      generated via a zip or war file in .elasticbeanstalk/config.yml in project folder
-----------------------------------------------------------------------------------------------:
ELASTIC BEANSTALK LIFECYCLE AND RDS:
  you can create an RDS instance within an EB environment:
    the instance is then linked to the EB environment
    you should only do this for really small scale dev and test projects
    environment properties:
      RDS_HOSTNAME, RDS_PORT, RDS_DB_NAME, RDS_USERNAME, RDS_PASSWORD
  you can create an RDS instance outside of EB:
    environment properties need to point at RDS instance
    data is outside of the EB env lifecycle
  
  how to decouple existing RDS instance within EB from EB environment:
    create an RDS snapshot:
    enable "delete protection":
    create a new eb environment with the same app version:
    ensure new environment can connect to the DB, (sg and env properties):
    swap environments (CNAME or DNS):
    remove sg rule from old environment:
    terminate the old environment:
    locate DELETE_FAILED stack, manually delete and select 'retain stuck resources':

  beanstalk lifecycle policies:
    can be based on:
      time, space
    can store at most 1000 versions:
    select option not to delete the source bundle in s3 to prevent data loss:

-----------------------------------------------------------------------------------------------:
CUSTOMIZING VIA .EBEXTENSIONS:
  allow you to customize EB environments:
  implements Cloudformation to implement changes:
  requirements:
    .ebextensions/ directory in the root of source code:
    extensions file needs to be in YAML or JSON format with .config extension:
    sections:
      option_settings:
        allows you to set options of resources
      resources:
        allows you to create new resources
        resources managed by this file will be deleted if environment is deleted
-----------------------------------------------------------------------------------------------:
EB and HTTPS:
  you need to apply the SSL cert to the load balancer directly (2 options):
    via EB console
    via .ebextensions/securelistener-alb.config, and remember to configure security group

-----------------------------------------------------------------------------------------------:
EB ENVIRONMENT CLONING:
  allows you to create a new environment by cloning an existing one:
  copies all changes you've made to environment via ebs console, but not unmanaged changes you've made via cli or api:
  you can clone via eb console, API, or "eb clone environmentName":
  when copying RDS, the instance is copied but data is not:
  different platform branches (languages eb supports) are not guaranteed to be compatible

-----------------------------------------------------------------------------------------------:
EB AND DOCKER (KNOW ALL):
  single container mode:
    implement when you want to only run one container on one docker host per environment:
    uses ec2 with docker, not ECS
    referred to as docker in console UI
    you provide 1 of these:
      dockerfile:
        eb will build a docker image and use this to run a container
      Dockerrun.aws.json (version 1):
        configures what image to use, ports, volumes, and other attributes
      docker-compose.yml:
        if you use docker compose
  multi-container mode:
    implements multiple containers running on one or more ec2 instances per environment
    implements an elb
    creates an ecs cluster:
      contains ec2 instances with containers 
    requires a dockerrun.aws.json (v2) file:
      in application source bundle and creates an environment
    requires images to be stored in a container registry such as ECR:

-----------------------------------------------------------------------------------------------:
env.yaml:
  environment configurations for elastic beanstalk:
cron.yaml:
  you can define periodic tasks and add to source bundle to add jobs to your worker's environment's queue automatically:

-----------------------------------------------------------------------------------------------:
PRACTICE TEST QUESTIONS (KNOW ALL):

-----------------------------------------------------------------------------------------------:
LAMBDA:

to include lambda code within a cfn template:
  implement Zipfile field ( inside AWS::Lambda::Function resource):
    "code here"

lambda concurrent executions:
  refers to the number of executions of your function code that is happening at same time
  push based event sources:
    concurrent executions = (invocations per second) x (average execution duration in seconds):
    by default the limit per region is 1000 concurrent executions
    aws enforces the unreserved concurrency pool will always be at least 100 concurrent executions 
    you can set a reserved concurrency per function as long as it does not exceed the limit - unreserved concurrency pool (900 by default)
    any additional requests beyond the limit will be rejected with a "TooManyRequestsException" error.
  poll based event sources:
    concurrent executions = at most the number of shards in a kinesis data stream

errors you might see when creating lambda functions via cli with CreateFunction API:
  InvalidParameterValueException:
    one of the parameters in the request is invalid
  ServiceException:
    the lambda service encountered an internal error

Invoke api for lambda:
  three types for invocationType attribute:
    RequestResponse:
      default, invokes the function synchronously
    Event:
      invokes the function asynchronously
    DryRun:
      validates parameter values and verifies that the user or role has permission to invoke fn

lambda authorizers:
  api gateway feature that allows a lambda function to control access to your api
  token based:
    receives the caller's identity in a bearer token
  request parameter-based:
    receives the caller's identity in combination of headers, query string params, and variables
cognito authorizers:
  api gateway feature that allows cognito to control access to your api
  implementation:
    create an user pool
    create authorizer in api gateway using the cognito user pool id
    set the name of the header that will be used from the request to the user pool as a token source for authorization

-----------------------------------------------------------------------------------------------:
AWS SAM, serverless application model:
  toolkit that improves the developer experience of building and running serverless applications on aws
  benefits:
    defines your application infrastructure code quickly, using less code
    manage applications through their entire dev lifecycle
    quickly provision permissions between resources with sam connectors
    manage your terraform serverless applications
  to allow cloudformation to use SAM syntax in template:
    include Transform section:
      specifies the SAM version to use
  architecture:
    sam template specification:
      framework that you can use to define your serverless application infrastructure on AWS
      you can implement both the cloudformation and sam syntax within the same template
      implements cloudformation
    sam cli:
      command line tool used with sam templates and third party integrations such as terraform to build and run your serverless apps
      sam init:
        initilizes a serverless application with an aws sam template
      sam build:
        builds any dependencies that your application has
      sam package:
        this command zips your code artifacts, uploads them to s3, and produces a packaged aws sam template file that's ready to be used
      sam deploy:
        this command uses the sam package file to deploy your application
    alternative to sam package and sam deploy:
      aws cloudformation package and aws cloudformation deploy
    aws serverless application repository, SAR:
      allows you to share your serverless applications packages using SAM with other aws accounts
    sam cli + aws toolkits:
      allows you to debug your lambda fns locally and execute code line by line

-----------------------------------------------------------------------------------------------:
API GATEWAY:

Exponential backoff:
  use whenever you get ThrottlingExceptionError:
    you'll get this error when you use too many api calls for your api limits
  functionality is automatically implemented in sdk api calls
  implement yourself when:
    you receive 500 server errors
    throttling issues

how to invalidate api gateway cache:
  from the ui:
    can be invalidated immediately
  client can invalidate cache via 'cache-control: max-age=0' header:
    allows you to bypass cache and retrieve data from source
    needs proper authorization:
      select the 'require authorization' checkbox in the cache settings of your api via the console
      execute-api:InvalidateCache action:
        allows iam role to make authorized invalidate cache requests to the api

aws_iam method authorization type for api gateway:
  allows you to limit the api to only be accessed by iam users or roles

https status code errors in api gateway:
  504:
    integration failure, integration timeout, the integration timeout lasts from 50ms to 29 seconds for all integration types:
  502 or 429:
    too many requests and api gateway is throttling

lambda and codedeploy:
  codedeploy can help you automate traffic shift for lambda aliases via the SAM framework
-----------------------------------------------------------------------------------------------:
DYNAMODB:

dynamodb errors:
  ProvisionedThroughputExceededException:
    not enough RCU or WCU left for operation
  ThrottlingException:
    rate of requests exceeds the allowed throughput
  RequestLimitExceeded:
    provisioned RCU or WCU (throughput) exceeds current throughput limit for account

dynamodb operations:
  BatchGetItem:
    allows you to get multiple items at once in parallel
  BatchWriteItem:
    allows you to write multiple items at once in parallel
  UpdateItem:
    allows you to perform write requests
    you can implement an atomic counter
  TransactWriteItems or TransactGetItems:
    allows you to group multiple actions together and submit them as a single all-or-nothing operation
  for each of these operations you can include 'ReturnConsumedCapacity' argument:
    to return the number of wcu consumed by the operation
    additional arguments:
      total:
        returns the total number of wcus consumed
      indexes:
        returns the total number of wcus consumed, with subtotals for the table and any secondary indexes that were affected by operation
      none:
        no wcu details are returned (default)
  table cleanup:
    scan and deleteItem:
      very slow and is expensive as it consumes rcu and wcu
    drop table and recreate table:
      recommended as it is fast and efficient
  copying a dynamodb table:
    backup and restore into a new table:
      recommended but takes some time
    use aws data pipeline:
    scan and implement PutItem or BatchWriteItem:
      implement your own code 

  projection expression:
    allows you to get just some of dynamodb attributes rather than all of them when reading data from table
  filter expressions:
    allows you to get just some of dynamodb items within the results returned to you when reading data from table
  error retries and exponential backoff:
    uses progressively longer waits between retries for consecutive error responses 
  
dynamodb write types:
  concurrent:
    sequential writes overwrite preceding write
  conditional writes:
    each item has an attribute that acts as a version number and you can update item only if the version number on the server side has not changed
  atomic writes:
    increase value incrementally by 1
  batch writes:
    many writes/updates at a time

dynamodb locking types:
  optimistic locking:
    prevents stale writes with no drop in performance
    each item has an attribute that acts as a version number and you can update item only if the version number on the server side has not changed
    implement with conditional writes:
      only updates if condition is true:
  pestimistic locking:
    prevents stale writes with significant drops in performance
  overly optimistic locking:
    prevents stale writes with a system that implements only one user

to avoid hot partitioning:
  implement dynamodb write sharding:
    if you have two partition key values that will be used over and over, add random suffixes to partition key to allow a better distribution of items evenly across partitions

dynamodb patterns with s3:
  large objects:
    store large objects inside s3, index each item inside a dynamodb table that stores a reference to object
    client can query the metadata via an api on top of dynamodb or just query dynamodb directly if they have the partition key

dynamodb access control:
  web identity federation or cognito identity pools:
    each user gets temp aws credentials
  iam role with conditions:
    LeadingKeys:
      this condition allows a user to only access items that are associated with their partition key
    attributes:
      limits specific attributes the user can see

EventBridge rules and dynamodb:
  rules are not capable of detecting table level events from dynamodb

kinesis adapter:
  recommended way to consume streams from dynamodb for real time processing of data

-----------------------------------------------------------------------------------------------:
Logs in RDS:
  by default error logs are published to cloudwatch
  you can also enable slow query logs and audit logs

ecs task placement process:
  ecs identifies the instances that satisfy requirements in the task definition (has to have capabilities for computing power, memory, etc):
  ecs identifies the instances that satisfy the task placement constraints:
  ecs identifies the instances that satisfy the task placement strategies:
  ecs then selects the instances for task placement
ecs task placement constraints:
  distinctInstance constraint:
    places each task on a different container instance
  memberOf:
    places task on instances that satisfy an expression, using cluster query language
    cluster query language:
      expressions that enable you to group objects, like container instances by a specific attribute
ecs task placement strategies (for ec2 mode only):
  algorithms for selecting instances for task placement or tasks for termination:
  binpack:
    places tasks based on filling a container before placing tasks on another instance:
    saves cpu and memory:  
    minimizes the number of instances in use.
  random:
    places tasks randomly:
  spread:
    places tasks evenly across specified 'bins' given in the field attribute:
    ex...instanceId, ecs.availability-zone


cloudwatch detailed monitoring:
  feature of cloudwatch that allows you to enable high resolution metrics: 
  not enabled by default:
  monitor your resources more frequently and capture short-lived performance changes that may not be captured with basic monitoring.
  if enabled, allows you to capture data with a 1-minute frequency for EC2 instances, and a 1-second frequency for other AWS resources VS 5 minute frequencies for all resources

iam policy simulator:
  evaluates the policies that are present in an environment and returns whether the requested action would be allowed or denied:
dry run:
  checks whether you have the required permissions for the action, without actually making the request
  --dry-run parameter in cli:
    some cli commands contain this option to simulate api calls
    UnauthorizedOperation:
      lets us know we don't have the access permissions to run command
    DryRunOperation:
      lets us know we do have the access permissions to run command


amazon cognito sync:
  enables cross device syncing of application related user data for one user:
  you can use it to synchronoize user profile data across mobile devices and the web without requiring your own backend
  deprecated, recommended to use appsync instead:
AWS APPSYNC:
  serverless GraphQL and Pub/Sub API service that simplifies building modern web and mobile applications and allows you to securely access, manipulate, and combine data from one or more data sources:
  replaces cognito sync:
  you specify the data for your app and appsync manages everything needed to keep the app updated in real time including websockets:
  allows you to create custom domains and allows you to integrate with ACM to enable HTTPS communication
  can be integrated with DynamoDB to make is easy for you to build collaborative apps that keep shared data updated in real time

aws distro for opentelemetry:
  broader tracing solution that includes, xray and other tracing solutions
  ability to send traces to multiple different tracing backends simultaneously without having to reinstrument your code
  open source

cloudwatch metrics for api gateway:
  latency:
    measures the overall responsiveness of your api calls
  integrationLatency:
    measures the responsiveness of the backend.. not the requests which are served from the backend
  CacheHitCount:
    fetches the number of requests served from cache
  CacheMissCount:
    fetches the number of requests served from a backend in a given period

aws amplify:
  set of purpose built tools and features that enables frontend web and mobile developers to build full stack apps on aws:
  amplify hosting:
    proves git based workflow for hosting full stack serverless web apps w continuous deployment
    cypress tests:
      update build settings in the amplify.yml config file
  amplify studio:
    visual dev env that implements process where you build your frontend UI with a set of ready to use UI components, and create an app backend with aws resources 
  amplify libraries:
    connects your app to aws services

aws CodeStar:
  allows you to quickly develop, build, and deploy apps on aws:
  project dashboard contains app activity:
  integrated solution that contains:
    github, codecommit, codebuild, codedeploy, cloudformation, codepipeline, cloudwatch, jira, beanstalk, cloud9, lambda, and ec2

aws CDK, cloud development kit:
  open source software development framework to provision your cloud application resources using familiar programming languages (js, typescript, python, java, and .net)
  constructs:
    high level class libraries that allow you to provision resources

signing AWS API requests:
  all AWS API requests implement the HTTP api:
    SigV4 provides implementation for signing HTTP api requests
    if you use the SDK or CLI, these requests are signed for you

before a cloudformation template can be used by cloudformation, it must be uploaded to:
  s3, even if using the console it uploads to s3 under the hood and references that template

cloudwatch synthetics canary:
  configurable script that monitors your resources and reproduces what your customers do programmatically to find issues before customers are impacted
  
codeArtifact:
  artifact management ystem that helps you store and retrieve software dependencies
  implements common dependency management tools such as maven, gradle, npm, yarn, twine, pip, and nuget

amazon codeGuru:
  ml powered service that automates code reviews and application performance recommendations

amazon cloud9:
  cloud based ide:
  code editer, debugger, and terminal in a browser
  prepacked with essential tools for popular programming languages

nitro enclaves:
  allows you to process highly sensitive data in an isolated compute environment

to enforce ssl requests to objects stored inside s3 bucket:
  implement aws:SecureTransport iam action

Amazon OpenSearch Service:
  allows you to implement searches on any database for fields, or partial fields:
  amazon elasticsearch v2:
  can also perform analytic queries from data ingested by firehose, iot, and cloudwatch logs

Amazon Managed Streaming for Apache Kafka (Amazon MSK):
  provides an alternative to amazon kinesis to stream data:
  is a fully managed apache kafka on aws
  mfk serverless:
    allows you to run msk without managing the capacity
  differences from kinesis:
    higher message size limit, > 1 MB:
    unlimited time for data storage:

aws AppConfig:
  allows you to deploy dynamic configuration changes to your applications independently of any code deployments because the configurations are stored outside of the app in AppConfig

when deleting stacks with references to other stacks:
  delete the stacks that import the values first, and lastly delete the stack with outputs

commands to run when you want to pull existing docker images from ecr:
  (aws ecr get-login --no-include-email)
  docker pull 31431432.dkr.ecr.region/repositoryName:version

how to migrate a elastic beanstalk environment from account a to account b:
  create a saved configuration in team a's account and download it to local machine
  make the account specific parameter changes and upload to s3 bucket in account b
  from the elb console in account b, create an application from saved configurations

configuration file for ecs clusters:
  /etc/ecs/ecs.config

to retrieve an ip from a client that connects to an elb:
  retrieve the 'X-forwarded-for' header of the request

if s3 bucket is timing out when trying to list items:
  implement max-items parameter or page-size parameter to reduce number of objects returned
to implement more efficient reads on large dynamodb tables:
  reduce page size and implement query operations   
if cli script to list items is timing out:
  implement -max-items and -starting-token parameters

if you are getting 403 error when trying to upload large file with kms key to s3 bucket:
  check to ensure you have the kms:Decrypt and kms:GenerateDataKey* actions because these permissions are required when s3 must decrypt and read data to complete the multipart upload

difference between 's3:PostObject' and 's3:PutObject':
  both upload an object to bucket, but different method of uploading
  post:
    uploads object via HTML form
  put:
    uploads object via pre-signed url or s3 endpoint

KCL, Kinesis Client Library:
  ensures ec2 instances contain a record processer to process each shard and one KCL worker:
  number of instances <= number of shards:
  also tracks shards in the stream using a dynamodb table

how to give access to server in an on premise environment trying to access aws services:
  create a new IAM user with programmatic access via access keys
  create the credential file at ~/.aws/credentials
  it is always best practice to use role with ec2 instance, however when server is on prem then it can only access aws services via access keys

-----------------------------------------------------------------------------------------------:
  IAM access keys: 
  are long term credentials used with IAM users only:
  use access keys for authentication when using CLI:
  they don't change automatically or regularly:
  similar to user name and password, but differences are:
    an IAM user can have two access keys, either 0, 1, or 2
    can be created, deleted, made inactive or active
  formed from two parts: 
    access key ID: username
    secret access key: password

  aws configure: 
    allows us to configure the default configuration for CLI
  aws configure --profile nameOfProfile: 
    named profiles:
      allow us to configure multiple aws accounts to our CLI instead of just using one account:
    whenever running commands append '--profile nameOfProfile' to run from NameOfProfile account:

-----------------------------------------------------------------------------------------------:
EC2 OPTIMIZED INSTANCES (KNOW ALL):
  compute:
    great for compute intensive tasks that require high performance
    HPC, batch processing workloads, gaming server
  storage:
    great for storage intensive tasks that require high, sequential io access to large data sets on local storage
    implement for OLTP systems, relational databases cache for in memory databases, data warehousing apps, distributed file systems
  general purpose:
    implement for a diversity of workloads such as web servers or code repositories
  memory optimized instances:
    fast performance for workloads that process large data sets in memory (ram)
    implement for high performance or non relational dbs, apps performing real time processing of big unstructured data


-----------------------------------------------------------------------------------------------:
ELASTICACHE:
  in memory caching solution designed primarily for databases and applications running on ec2 instances:
  not persistent:
  use cases:
    implement for read heavy workloads with low latency requirements:
    allows externally hosted user session state, allowing stateless servers:
  using elasticache requires application code changes:
  can also be used with other aws services and applications as a caching layer:
  designed for high performance (implements in memory vs using disk like rds does):
  
  supports redis or memcached engines:
    redis:
      advanced data structures:
      multi AZ replication:
      backup and restore features:
      authenticate via using Redis AUTH command:
      replication (scale reads)
      transactions, multiple operations are treated as one
    memcached:
      simple data structures:
      no replication:
      no backups:
      multi threaded, higher performance when doing multi core computations:
      multiple nodes (sharding)

  strategies:
    lazy loading/ cache-aside/ lazy population:
      app requests data from cache
      if cache hit, then receives data
      if cache miss, the cache reads from the db, then writes that data to cache
      cons:
        cache miss penalty is costly
        stale data can be in cache because if data is updated in db, it is not updated in cache
    write through:
      whenever app writes to db:
        app writes to cache as well:
      if cache hit, app then receives data
      if cache miss, implement lazy loading
      pros:
        data in cache is never stale, reads are quick
        write penalty vs read penalty with lazy loading is a better ux
      cons:
        cache churn:
          a lot of the data will never be read if there are a lot of writes because everything will be written to cache as well
        existing db data is missing in cache until it is written/updated to db

  cache eviction:
    you can delete item explicitly
    you can set an item TTL
    if memory is full, the LRU (least recently used) items are deleted

  amazon memorydb for redis:
    redis compabtible, durable, in memory database service
    ultra fast performance, scales from 10 GBS to 100s TBs of storage
-----------------------------------------------------------------------------------------------:








  
     